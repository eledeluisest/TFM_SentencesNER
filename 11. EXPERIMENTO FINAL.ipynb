{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33272,"status":"ok","timestamp":1662962548739,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"kqy2Azu13cZD","outputId":"1a74887a-45d1-426b-bafc-895f5a4019b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 9.2 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n","Collecting boto3\n","  Downloading boto3-1.24.70-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 46.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n","Collecting botocore<1.28.0,>=1.27.70\n","  Downloading botocore-1.27.70-py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 43.9 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.70->boto3->pytorch-pretrained-bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 48.8 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.70->boto3->pytorch-pretrained-bert) (1.15.0)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.24.70 botocore-1.27.70 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=9a0e9535f37bea8cdace46616c582406a86d9ed083d917e4b667441792af12c3\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 7.0 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=f86e48a663354577afc8b43d06dea34ad8504f6e6764bfab78b6e3231d22e68a\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["# Instalaciones\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","!pip install langdetect"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RR3BSoh68TcV"},"outputs":[],"source":["import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33929,"status":"ok","timestamp":1662962582656,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"TAjLEmtS3e6i","outputId":"031a8515-8c2b-4e6f-f359-6afda9b05462"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 7.4 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}],"source":["# Importaciones\n","# GENERAL Y PREPROCESADO\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","import spacy\n","!python -m spacy download es_core_news_sm\n","import es_core_news_sm\n","from langdetect import detect\n","import os.path\n","import datetime\n","# BERT\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","from seqeval.metrics import f1_score\n","\n","\n","from IPython.display import clear_output "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23305,"status":"ok","timestamp":1662962605949,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"eoUlwBGi3hLp","outputId":"e96d7a67-38da-41fd-abf3-ff50101f6105"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Conexion a drive y descompresión de los corpus\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10719,"status":"ok","timestamp":1662962616658,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"IrHxMWw53hmD","outputId":"dcfcc49e-079d-449d-d2bf-6741b805863d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/COLAB - TFM/corpora-master.zip\n","370a87a85d1d24d418a86abbe1f6918383362467\n","   creating: corpora-master/\n","  inflating: corpora-master/.gitignore  \n","   creating: corpora-master/.vscode/\n","  inflating: corpora-master/.vscode/settings.json  \n","   creating: corpora-master/2021/\n","  inflating: corpora-master/2021/README.md  \n","   creating: corpora-master/2021/eval/\n","   creating: corpora-master/2021/eval/develop/\n","  inflating: corpora-master/2021/eval/develop/README.md  \n","   creating: corpora-master/2021/eval/develop/scenario1-main/\n"," extracting: corpora-master/2021/eval/develop/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/develop/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/testing/\n","  inflating: corpora-master/2021/eval/testing/README.md  \n","   creating: corpora-master/2021/eval/testing/scenario1-main/\n"," extracting: corpora-master/2021/eval/testing/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/testing/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/training/\n","   creating: corpora-master/2021/eval/training/scenario1-main/\n"," extracting: corpora-master/2021/eval/training/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/training/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/ref/\n","   creating: corpora-master/2021/ref/develop/\n","  inflating: corpora-master/2021/ref/develop/README.md  \n","  inflating: corpora-master/2021/ref/develop/cord.50.ann  \n","  inflating: corpora-master/2021/ref/develop/cord.50.txt  \n","  inflating: corpora-master/2021/ref/develop/medline.25.ann  \n","  inflating: corpora-master/2021/ref/develop/medline.25.txt  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.ann  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.txt  \n","   creating: corpora-master/2021/ref/testing/\n","  inflating: corpora-master/2021/ref/testing/README.md  \n","  inflating: corpora-master/2021/ref/testing/cord.150.ann  \n","  inflating: corpora-master/2021/ref/testing/cord.150.txt  \n","  inflating: corpora-master/2021/ref/testing/medline.75.ann  \n","  inflating: corpora-master/2021/ref/testing/medline.75.txt  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.ann  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.txt  \n","   creating: corpora-master/2021/ref/training/\n","  inflating: corpora-master/2021/ref/training/medline.1200.es.ann  \n","  inflating: corpora-master/2021/ref/training/medline.1200.es.txt  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.ann  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.txt  \n","   creating: corpora-master/2021/submissions/\n","   creating: corpora-master/2021/submissions/baseline/\n","   creating: corpora-master/2021/submissions/baseline/develop/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/2021/submissions/baseline/submission.zip  \n","   creating: corpora-master/2021/submissions/baseline/testing/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/LICENSE  \n","  inflating: corpora-master/README.md  \n","   creating: corpora-master/scripts/\n","  inflating: corpora-master/scripts/README.md  \n","  inflating: corpora-master/scripts/anntools.py  \n","  inflating: corpora-master/scripts/baseline.py  \n","  inflating: corpora-master/scripts/score.py  \n","  inflating: corpora-master/scripts/score_2021.py  \n"]}],"source":["!unzip /content/drive/MyDrive/COLAB\\ -\\ TFM/corpora-master.zip"]},{"cell_type":"markdown","metadata":{"id":"OSmY7PSj3myM"},"source":["# FUNCIONES"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_erLdrdK3oSC"},"outputs":[],"source":["def clasifica_data(file):\n","  file_csv = file\n","  ingles = []\n","  espanol = []\n","  with open(file_csv, \"r\") as f:\n","    texto = f.read().split(\"\\n\")\n","  for frase in texto:\n","    espaciado = frase.split(\" \")\n","    idioma = detect(frase)\n","    if idioma == \"en\":\n","      ingles.append(frase)\n","    elif idioma == \"es\":\n","      espanol.append(frase)\n","  return ingles, espanol\n","\n","  \n","def data_preprocessing_es(texto, ann = None):\n","  nlp = es_core_news_sm.load()\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","def data_preprocessing_en(texto, ann = None):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","def bert_formating(data, MAX_LEN, test_size=0.2, bs = 16):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                              random_state=2022, test_size=test_size)\n","  tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                              random_state=2022, test_size=test_size)\n","  tr_inputs = torch.tensor(tr_inputs)\n","  val_inputs = torch.tensor(val_inputs)\n","  tr_tags = torch.tensor(tr_tags)\n","  val_tags = torch.tensor(val_tags)\n","  tr_masks = torch.tensor(tr_masks)\n","  val_masks = torch.tensor(val_masks)\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, valid_data, valid_sampler,\\\n","            valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs\n","\n","def bert_formating_one_dataset(data, MAX_LEN, bs = 16, tag2idx = None, tags_vals=None,word2idx=None):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  if tags_vals is None:\n","    tags_vals = list(set(data[\"Tag\"].values))\n","  if tag2idx is None:\n","    tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words)\n","  if word2idx is None:\n","    word2idx = {w: i + 2 for i, w in enumerate(words)}\n","    word2idx[\"UNK\"] = 1\n","    word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  # tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","\n","  tokens_ids = []\n","  for s in tokenized_texts:\n","    tmp = []\n","    for w in s:\n","      try:\n","        tmp.append(word2idx[w])\n","      except:\n","        tmp.append(word2idx[\"UNK\"])\n","    tokens_ids.append(tmp)\n","  print(tokens_ids)\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs = torch.tensor(input_ids, dtype=torch.int64)\n","  tr_tags = torch.tensor(tags, dtype=torch.int64)\n","  tr_masks = torch.tensor(attention_masks, dtype = torch.uint8)\n","\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, tag2idx, device, tags_vals, idx2word, tr_inputs, word2idx\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = \"\"):\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()\n","  if reentreno:\n","    print(\"Loading existing model...\")\n","    model.load_state_dict(torch.load(save_path))\n","  if FULL_FINETUNING:\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'gamma', 'beta']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.01},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.0}\n","      ]\n","  else:\n","      param_optimizer = list(model.classifier.named_parameters()) \n","      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","  return model, optimizer\n","\n","def training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = 200,\n","             max_grad_norm = 1.0, save_path = \"./bert1\", early_stopping=15, use_scheduler=True):\n","  lmbda = lambda x: 0.98 if use_scheduler else 1\n","  scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","  train_f1 = []\n","  train_losses = []\n","  val_losses = []\n","  contador_max = -1\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","      # TRAIN loop\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          # add batch to gpu\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          # forward pass\n","          loss = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask, labels=b_labels)\n","          # backward pass\n","          loss.backward()\n","          # track train loss\n","          tr_loss += loss.item()\n","          nb_tr_examples += b_input_ids.size(0)\n","          nb_tr_steps += 1\n","          # gradient clipping\n","          # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","          # update parameters\n","          optimizer.step()\n","          model.zero_grad()\n","      scheduler.step()\n","      # print train loss per epoch\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","      # VALIDATION on validation set\n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels, true_inputs = [], [],[]\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch          \n","          with torch.no_grad():\n","              tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                    attention_mask=b_input_mask, labels=b_labels)\n","              logits = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          inputs = b_input_ids.to('cpu').numpy()\n","          \n","          true_inputs.append(inputs)\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.append(label_ids)\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          eval_loss += tmp_eval_loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","          \n","          nb_eval_examples += b_input_ids.size(0)\n","          nb_eval_steps += 1\n","      eval_loss = eval_loss/nb_eval_steps\n","      print(\"Validation loss: {}\".format(eval_loss))\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","      valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","      valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","      f1 = f1_score([pred_tags], [valid_tags])\n","      train_f1.append(f1)\n","      train_losses.append(tr_loss/nb_tr_steps)\n","      val_losses.append(eval_loss)\n","\n","      max_f1 = max(train_f1)\n","      if f1 == max_f1:\n","        contador_max = 1\n","        torch.save(model.state_dict(), save_path)\n","      if contador_max > 0:\n","        contador_max += 1\n","      print(\"F1-Score: \" + str(train_f1[-1]))\n","      if round(max_f1, 2) > 0.1 and contador_max > early_stopping :\n","        print(\"Early stopping...\")\n","        return 0\n","def training_scheduler(model, optimizer, loss_fn, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs=200,\n","             max_grad_norm=1.0, save_path=\"./bert1\", early_stopping=15, print_freq=10, use_scheduler=True):\n","    lmbda = lambda x: 0.98 if use_scheduler else 1\n","    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","\n","    train_f1 = []\n","    train_losses = []\n","    val_losses = []\n","    contador_max = -1\n","\n","    for ep in range(epochs):\n","        # TRAIN loop\n","        model.train()\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(train_dataloader):\n","            # add batch to gpu\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # forward pass\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","            loss = model.compute_loss(loss_fn, logits, b_labels, b_input_mask)\n","            # backward pass\n","            loss.backward()\n","            # track train loss\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","            # gradient clipping\n","            # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","            # update parameters\n","            optimizer.step()\n","            model.zero_grad()\n","        scheduler.step()\n","        # print train loss per epoch\n","        if ep % print_freq == 0:\n","            print(f\"{ep}/{epochs}: Train loss: {tr_loss / nb_tr_steps}\")\n","\n","        # VALIDATION on validation set\n","        model.eval()\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        predictions, true_labels, true_inputs = [], [], []\n","        for batch in valid_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            with torch.no_grad():\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","            tmp_eval_loss = model.compute_loss(loss_fn, logits, b_labels, b_input_mask)\n","\n","            logits = logits.cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            inputs = b_input_ids.to('cpu').numpy()\n","\n","            true_inputs.append(inputs)\n","            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","            true_labels.append(label_ids)\n","\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","            eval_loss += tmp_eval_loss.item()\n","            eval_accuracy += tmp_eval_accuracy\n","\n","            nb_eval_examples += b_input_ids.size(0)\n","            nb_eval_steps += 1\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        if ep % print_freq == 0:\n","            print(\"\\tValidation loss: {}\".format(eval_loss))\n","            print(\"\\tValidation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n","        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","        valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","        valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in true_inputs for l_i in l]\n","        f1 = f1_score([pred_tags], [valid_tags])\n","        train_f1.append(f1)\n","        train_losses.append(tr_loss / nb_tr_steps)\n","        val_losses.append(eval_loss)\n","\n","        max_f1 = max(train_f1)\n","        if f1 == max_f1:\n","            contador_max = 1\n","            torch.save(model.state_dict(), save_path)\n","        if contador_max > 0:\n","            contador_max += 1\n","        if ep % print_freq == 0:\n","            print(f\"\\tF1-Score: {train_f1[-1]}\\n\")\n","\n","        if round(max_f1, 2) > 0.1 and contador_max > early_stopping:\n","            print(\"We should apply early stopping now\")\n","\n","def evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = \"./bert1\",\n","             guarda_resultado=\"/content/drive/MyDrive/COLAB - TFM/resultado_entrenamiento1.csv\"):\n","  #evaluate model\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  model.load_state_dict(torch.load(save_path))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()  \n","  model.eval()\n","  predictions = []\n","  true_labels = []\n","  true_inputs = []\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","  print(len(valid_dataloader))\n","  for batch in tqdm(valid_dataloader):\n","      #print(len(batch))\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","\n","      with torch.no_grad():\n","          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                attention_mask=b_input_mask, labels=b_labels)\n","          logits = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","          \n","      logits = logits.detach().cpu().numpy()\n","      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      label_ids = b_labels.to('cpu').numpy()\n","      inputs = b_input_ids.to('cpu').numpy()\n","      true_inputs.append(inputs)\n","      \n","      \n","      true_labels.append(label_ids)\n","      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","\n","  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n","  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n","  valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","\n","  count = 0 \n","  all_data = 0\n","  registro_resultado = []\n","  for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n","      for k,l,kk in zip(i,j,ll):\n","        count += 1\n","        registro_resultado.append([k,l,idx2word[kk.item()]])\n","        all_data += 1\n","  df_alldata = pd.DataFrame(registro_resultado)\n","  df_alldata.columns = [\"pred_tags\", \"valid_tags\", \"val_inputs\"]\n","  df_alldata[df_alldata[\"val_inputs\"] != \"PAD\"].\\\n","        to_csv(guarda_resultado,sep=\";\", encoding = \"utf-8\")\n","  return pred_tags, valid_tags, valid_inputs\n"]},{"cell_type":"markdown","metadata":{"id":"sK5ZChy33uc2"},"source":["#Entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTQ_6vvJ14KI"},"outputs":[],"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from math import floor, ceil\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WFEg4_4v2SEC"},"outputs":[],"source":["\n","num_reg = [150]\n","raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","df_cord = pd.read_csv(raiz+\"cord_resumen_oraciones_test.csv\", index_col=0)\n","df_wikinews = pd.read_csv(raiz+\"wikinews_resumen_oraciones_test.csv\", index_col=0)\n","df_medline = pd.read_csv(raiz+\"medline_resumen_oraciones_test.csv\", index_col=0)\n","\n","df_cord, df_cord_dev = train_test_split(df_cord, test_size=0.2)\n","df_wikinews, df_wikinews_dev = train_test_split(df_wikinews, test_size=0.2)\n","df_medline, df_medline_dev = train_test_split(df_medline, test_size=0.2)\n","df_cord_dev[\"nombre_fichero\"] = \"cord\"\n","df_wikinews_dev[\"nombre_fichero\"] = \"wikinews\"\n","df_medline_dev[\"nombre_fichero\"] = \"medline\"\n","df_dev = pd.concat([df_cord_dev, df_wikinews_dev, df_medline_dev], axis=0).reset_index(drop=True)\n","N_ITERACIONES = 5\n","df_dev[[\"Sentence #\", \"fichero\", \"nombre_fichero\"]].to_csv(raiz+\"dataset_dev_test.csv\")\n","# range(N_ITERACIONES)\n","for iteracion in [0,1]:\n","  for reg in num_reg:  \n","    lim_i_vw = floor(reg/3/2)\n","    lim_f_vw = ceil(reg/3/2)\n","    lim_i = floor(reg/3)\n","    lim_f = ceil(reg/3)\n","    print(reg,lim_i_vw,  lim_f_vw, lim_i, lim_f)\n","\n","    ### dif_wv_33\n","    df_cord_wv_1 = df_cord.sort_values(\"dif_wv_33\", ascending=False).head(lim_i_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv_2 = df_cord.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv = pd.concat([df_cord_wv_1, df_cord_wv_2], axis=0).reset_index(drop=True)\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv_1 = df_medline.sort_values(\"dif_wv_33\", ascending=False).head(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv_2 = df_medline.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv = pd.concat([df_medline_wv_1, df_medline_wv_2], axis=0).reset_index(drop=True)\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv_1 = df_wikinews.sort_values(\"dif_wv_33\", ascending=False).head(lim_i_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv_2 = df_wikinews.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv = pd.concat([df_wikinews_wv_1, df_wikinews_wv_2], axis=0).reset_index(drop=True)\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_wv = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_wv.to_csv(raiz+str(reg)+\"_dif_wv_33_test.csv\")\n","\n","    ### sum_freq_palabra\n","    df_cord_wv = df_cord.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_f)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_freq_palabra = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_freq_palabra.to_csv(raiz+str(reg)+\"_sum_freq_palabra_test.csv\")\n","\n","    ### sum_freq_pos\n","    df_cord_wv = df_cord.sort_values(\"sum_freq_pos\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sort_values(\"sum_freq_pos\", ascending=False).head(lim_f)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sort_values(\"sum_freq_pos\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_freq_pos = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_freq_pos.to_csv(raiz+str(reg)+\"_sum_freq_pos_test.csv\")\n","\n","    ### random\n","    df_cord_wv = df_cord.sample(min(lim_i, len(df_cord) ))[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sample(min(lim_i, len(df_medline_wv) ))[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sample(min(lim_i, len(df_wikinews_wv) ))[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_random = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_random.to_csv(raiz+str(reg)+\"_random_test.csv\")\n","\n","    print(len(dataset_wv), len(dataset_freq_palabra), len(dataset_freq_pos), len(dataset_random))\n","\n","    sufijo = \"_preprocessing.csv\"\n","    archivos = [\"wikinews.25.txt\",\"wikinews.75.txt\",\"wikinews.300.es.txt\",\n","                \"medline.25.txt\",\"medline.75.txt\",\"medline.1200.es.txt\",\n","                \"cord.50.txt\",\"cord.150.txt\"]\n","    for i, archivo in enumerate(archivos):\n","      if i == 0:\n","        df = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","        df[\"fichero\"] = archivo.split(\".\")[1]\n","        df[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","      else:\n","        df_tmp = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","        df_tmp[\"fichero\"] = archivo.split(\".\")[1]\n","        df_tmp[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","        df = pd.concat([df,df_tmp], axis=0).reset_index(drop=True)\n","    lim_i, lim_f = (-1, 51)\n","    # lim_i, lim_f = (25, 50)\n","\n","    df_dev_seleccion = pd.read_csv(raiz+\"dataset_dev.csv\", index_col=0)\n","    df[\"fichero\"] = df[\"fichero\"].astype(int)\n","    df_dev = pd.merge(df, df_dev_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","    print(len(df_dev))\n","    df_dev = df_dev.loc[(df_dev[\"longitud\"] > lim_i) & (df_dev[\"longitud\"] <= lim_f), :].copy()\n","    df_dev[\"Sentence #\"] = df_dev[\"Sentence #\"].astype(str) + \"_\" + df_dev[\"fichero\"].astype(str)  + \"_\" + df_dev[\"nombre_fichero\"]\n","    print(len(df_dev))\n","    df_ingles_dev = df_dev.loc[df_dev[\"nombre_fichero\"] == \"cord\"]\n","    df_espanol_dev = df_dev.loc[df_dev[\"nombre_fichero\"] != \"cord\"]\n","    # Seleccionar las oraciones que forman cada dataframe. '_random.csv'\n","    sufijos = [ '_random.csv','_sum_freq_palabra.csv','_sum_freq_pos.csv', '_dif_wv_33.csv']\n","    BS = 32\n","    NUM_EPOCHS = 450\n","    for suf in sufijos:\n","      num = reg\n","      resultado = \"resultado_\"+str(num)+suf\n","      df_seleccion = pd.read_csv(raiz+str(num)+suf, index_col=0)\n","      df[\"fichero\"] = df[\"fichero\"].astype(int)\n","      df_train = pd.merge(df, df_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","      print(len(df_train))\n","      df_train = df_train.loc[(df_train[\"longitud\"] > lim_i) & (df_train[\"longitud\"] <= lim_f), :].copy()\n","      df_train[\"Sentence #\"] = df_train[\"Sentence #\"].astype(str) + \"_\" + df_train[\"fichero\"].astype(str) + \"_\" + df_train[\"nombre_fichero\"]\n","      print(len(df_train))\n","      df_ingles_analiza = df_train.loc[df_train[\"nombre_fichero\"] == \"cord\"]\n","      df_espanol_analiza = df_train.loc[df_train[\"nombre_fichero\"] != \"cord\"]\n","\n","      # Entrenar con los distintos dataframes\n","      modelo_en = str(num) + \"_bert_en\" + str(lim_f) + suf.split(\".\")[0] + \"_iter\"+str(iteracion)\n","      modelo_es = str(num) + \"_bert_es\" + str(lim_f)  + suf.split(\".\")[0] + \"_iter\"+str(iteracion)\n","      raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","      log_file = \"/content/drive/MyDrive/COLAB - TFM/full_training\"+ suf \n","      df_cruce_ingles = df_ingles_analiza\n","      df_cruce_espanol = df_espanol_analiza\n","      for data, modelo, MAX_LEN, lan, data_dev in ([df_cruce_espanol.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]], raiz + modelo_es, lim_f, \"es\", df_espanol_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]],                                \n","                                    [df_cruce_ingles.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],raiz+modelo_en, lim_f, \"en\", df_ingles_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]]):\n","        with open(log_file, \"a+\") as f:\n","          f.write(\";\".join([str(datetime.datetime.now()), suf, modelo, \"\\n\"]))\n","        if len(data) > 0:\n","          print(modelo)\n","          print(len(data))\n","          tag2idx = {'B-Reference': 0, 'I-Concept': 1, 'O': 2, 'I-Predicate': 3, 'B-Action': 4, 'B-Predicate': 5, 'B-Concept': 6, 'I-Action': 7}\n","          tags_vals = ['B-Reference', 'I-Concept', 'O', 'I-Predicate', 'B-Action', 'B-Predicate', 'B-Concept', 'I-Action']\n","          train_data, train_sampler, train_dataloader, tag2idx_tr, device, tags_vals_tr, idx2word_tr, tr_inputs, word2idx = bert_formating_one_dataset(data, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS)\n","          print(tag2idx, tags_vals)\n","          valid_data, valid_sampler, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, word2idx = bert_formating_one_dataset(data_dev, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS,word2idx=word2idx)\n","          print(tag2idx, tags_vals)\n","          if os.path.exists(modelo):\n","            with open(log_file, \"a+\") as f:\n","              f.write(\"Escojo modelo existente: \" +  modelo +\"\\n\")\n","            model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = modelo)\n","          else:\n","            model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True)\n","          training(model, optimizer,train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = NUM_EPOCHS,\n","                      max_grad_norm = 1.0, save_path = modelo, early_stopping=NUM_EPOCHS)\n","          pred_tags, valid_tags, valid_inputs = evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n","                      guarda_resultado=raiz+resultado+\"_validacion_\"+str(MAX_LEN) + \"_iter\"+str(iteracion)+\"_\"+lan+\".csv\")\n","          pred_tags, valid_tags, valid_inputs = evaluate(model, train_dataloader, tag2idx_tr, device, tags_vals_tr, idx2word_tr, tr_inputs, save_path = modelo,\n","                      guarda_resultado=raiz+resultado+\"_entrenamiento_\"+str(MAX_LEN) + \"_iter\"+str(iteracion)+\"_\"+lan+\".csv\")\n","          os.system(\"zip \"+modelo)\n","        else:\n","          print(\"No hay datos\")\n","      clear_output()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvmNDdPy5WoD"},"outputs":[],"source":["!kill $(ps aux | awk '{print $2}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jyNrCTIi8ie9"},"outputs":[],"source":["print(\"hola\")"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyOKyzWaAyH34l3fCxlVdpHq"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}