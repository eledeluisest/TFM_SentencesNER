{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17614,"status":"ok","timestamp":1663180142216,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"VVVo-N22oBEI","outputId":"9d8b7abb-3bd0-48ae-ba93-e5cf7e93d758"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.24.72)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.1)\n","Requirement already satisfied: torch\u003e=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch\u003e=0.4.1-\u003epytorch-pretrained-bert) (4.1.1)\n","Requirement already satisfied: botocore\u003c1.28.0,\u003e=1.27.72 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch-pretrained-bert) (1.27.72)\n","Requirement already satisfied: jmespath\u003c2.0.0,\u003e=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch-pretrained-bert) (1.0.1)\n","Requirement already satisfied: s3transfer\u003c0.7.0,\u003e=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3-\u003epytorch-pretrained-bert) (0.6.0)\n","Requirement already satisfied: python-dateutil\u003c3.0.0,\u003e=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.28.0,\u003e=1.27.72-\u003eboto3-\u003epytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: urllib3\u003c1.27,\u003e=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore\u003c1.28.0,\u003e=1.27.72-\u003eboto3-\u003epytorch-pretrained-bert) (1.25.11)\n","Requirement already satisfied: six\u003e=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil\u003c3.0.0,\u003e=2.1-\u003ebotocore\u003c1.28.0,\u003e=1.27.72-\u003eboto3-\u003epytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch-pretrained-bert) (2022.6.15)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-\u003epytorch-pretrained-bert) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: numpy\u003e=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn\u003e=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (3.1.0)\n","Requirement already satisfied: joblib\u003e=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (1.1.0)\n","Requirement already satisfied: scipy\u003e=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn\u003e=0.21.3-\u003eseqeval) (1.7.3)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: langdetect in /usr/local/lib/python3.7/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n"]}],"source":["# Instalaciones\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","!pip install langdetect\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1663180142217,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"RGpkL5nuip8-"},"outputs":[],"source":["# ANALISIS\n","import gensim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"ObE8soCcoS9M"},"outputs":[{"name":"stdout","output_type":"stream","text":["2022-09-14 18:30:59.470322: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: spacy\u003c3.5.0,\u003e=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: pathy\u003e=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,\u003c1.10.0,\u003e=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: tqdm\u003c5.0.0,\u003e=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: numpy\u003e=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: thinc\u003c8.2.0,\u003e=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: langcodes\u003c4.0.0,\u003e=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: typing-extensions\u003c4.2.0,\u003e=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: cymem\u003c2.1.0,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: wasabi\u003c1.1.0,\u003e=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: preshed\u003c3.1.0,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: spacy-loggers\u003c2.0.0,\u003e=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: murmurhash\u003c1.1.0,\u003e=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: typer\u003c0.5.0,\u003e=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: catalogue\u003c2.1.0,\u003e=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: requests\u003c3.0.0,\u003e=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: spacy-legacy\u003c3.1.0,\u003e=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: srsly\u003c3.0.0,\u003e=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue\u003c2.1.0,\u003e=2.0.6-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,\u003e=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging\u003e=20.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open\u003c6.0.0,\u003e=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy\u003e=0.3.5-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.7/dist-packages (from requests\u003c3.0.0,\u003e=2.13.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: blis\u003c0.8.0,\u003e=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc\u003c8.2.0,\u003e=8.1.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click\u003c9.0.0,\u003e=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer\u003c0.5.0,\u003e=0.3.0-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe\u003e=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2-\u003espacy\u003c3.5.0,\u003e=3.4.0-\u003ees-core-news-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","[=================================-----------------] 66.5% 1105.5/1662.8MB downloaded"]}],"source":["# Importaciones\n","# GENERAL Y PREPROCESADO\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","import spacy\n","!python -m spacy download es_core_news_sm\n","import es_core_news_sm\n","from langdetect import detect\n","import os.path\n","import datetime\n","# BERT\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","from seqeval.metrics import f1_score\n","\n","import gensim.downloader\n","word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":426539,"status":"aborted","timestamp":1663180681684,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"nu02Cu8roWKV"},"outputs":[],"source":["# Conexion a drive y descompresión de los corpus\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":426641,"status":"aborted","timestamp":1663180682120,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"D6itlXjaoZFp"},"outputs":[],"source":["!unzip /content/drive/MyDrive/COLAB\\ -\\ TFM/corpora-master.zip"]},{"cell_type":"markdown","metadata":{"id":"pqugLk_Lzyms"},"source":["### Funciones"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":425702,"status":"aborted","timestamp":1663180682120,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"wEwSrIk1oaaO"},"outputs":[],"source":["def clasifica_data(file):\n","  file_csv = file\n","  ingles = []\n","  espanol = []\n","  with open(file_csv, \"r\") as f:\n","    texto = f.read().split(\"\\n\")\n","  for frase in texto:\n","    espaciado = frase.split(\" \")\n","    idioma = detect(frase)\n","    if idioma == \"en\":\n","      ingles.append(frase)\n","    elif idioma == \"es\":\n","      espanol.append(frase)\n","  return ingles, espanol\n","\n","  \n","def data_preprocessing_es(texto, ann = None):\n","  nlp = es_core_news_sm.load()\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) \u003e 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea \u003e 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) \u003c int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() \u003c= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","def data_preprocessing_en(texto, ann = None):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) \u003e 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea \u003e 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) \u003c int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() \u003c= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","def bert_formating(data, MAX_LEN, test_size=0.2):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  bs = 16\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) \u003e MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i\u003e0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                              random_state=2022, test_size=test_size)\n","  tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                              random_state=2022, test_size=test_size)\n","  tr_inputs = torch.tensor(tr_inputs)\n","  val_inputs = torch.tensor(val_inputs)\n","  tr_tags = torch.tensor(tr_tags)\n","  val_tags = torch.tensor(val_tags)\n","  tr_masks = torch.tensor(tr_masks)\n","  val_masks = torch.tensor(val_masks)\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, valid_data, valid_sampler,\\\n","            valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = \"\"):\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()\n","  if reentreno:\n","    print(\"Loading existing model...\")\n","    model.load_state_dict(torch.load(save_path))\n","  if FULL_FINETUNING:\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'gamma', 'beta']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.01},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.0}\n","      ]\n","  else:\n","      param_optimizer = list(model.classifier.named_parameters()) \n","      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","  return model, optimizer\n","\n","def training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = 200,\n","             max_grad_norm = 1.0, save_path = \"./bert1\"):\n","  train_f1 = []\n","  train_losses = []\n","  val_losses = []\n","  contador_max = -1\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","      # TRAIN loop\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          # add batch to gpu\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          # forward pass\n","          loss = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask, labels=b_labels)\n","          # backward pass\n","          loss.backward()\n","          # track train loss\n","          tr_loss += loss.item()\n","          nb_tr_examples += b_input_ids.size(0)\n","          nb_tr_steps += 1\n","          # gradient clipping\n","          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","          # update parameters\n","          optimizer.step()\n","          model.zero_grad()\n","      # print train loss per epoch\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","      # VALIDATION on validation set\n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels, true_inputs = [], [],[]\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          with torch.no_grad():\n","              tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                    attention_mask=b_input_mask, labels=b_labels)\n","              logits = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          inputs = b_input_ids.to('cpu').numpy()\n","          \n","          true_inputs.append(inputs)\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.append(label_ids)\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          eval_loss += tmp_eval_loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","          \n","          nb_eval_examples += b_input_ids.size(0)\n","          nb_eval_steps += 1\n","      eval_loss = eval_loss/nb_eval_steps\n","      print(\"Validation loss: {}\".format(eval_loss))\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","      valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","      valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","      f1 = f1_score([pred_tags], [valid_tags])\n","      train_f1.append(f1)\n","      train_losses.append(tr_loss/nb_tr_steps)\n","      val_losses.append(eval_loss)\n","\n","      max_f1 = max(train_f1)\n","      if f1 == max_f1:\n","        contador_max = 1\n","        torch.save(model.state_dict(), save_path)\n","      if contador_max \u003e 0:\n","        contador_max += 1\n","      print(\"F1-Score: \" + str(train_f1[-1]))\n","      if round(max_f1, 2) \u003e 0.1 and contador_max \u003e 10 :\n","        print(\"Early stopping...\")\n","        return 0\n","def evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = \"./bert1\",\n","             guarda_resultado=\"/content/drive/MyDrive/COLAB - TFM/resultado_entrenamiento1.csv\"):\n","  #evaluate model\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  model.load_state_dict(torch.load(save_path))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()  \n","  model.eval()\n","  predictions = []\n","  true_labels = []\n","  true_inputs = []\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","  print(len(valid_dataloader))\n","  for batch in tqdm(valid_dataloader):\n","      #print(len(batch))\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","\n","      with torch.no_grad():\n","          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                attention_mask=b_input_mask, labels=b_labels)\n","          logits = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","          \n","      logits = logits.detach().cpu().numpy()\n","      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      label_ids = b_labels.to('cpu').numpy()\n","      inputs = b_input_ids.to('cpu').numpy()\n","      true_inputs.append(inputs)\n","      \n","      \n","      true_labels.append(label_ids)\n","      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","\n","  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n","  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n","  valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","\n","  count = 0 \n","  all_data = 0\n","  registro_resultado = []\n","  for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n","      for k,l,kk in zip(i,j,ll):\n","        count += 1\n","        registro_resultado.append([k,l,idx2word[kk.item()]])\n","        all_data += 1\n","  df_alldata = pd.DataFrame(registro_resultado)\n","  df_alldata.columns = [\"pred_tags\", \"valid_tags\", \"val_inputs\"]\n","  df_alldata[df_alldata[\"val_inputs\"] != \"PAD\"].\\\n","        to_csv(guarda_resultado,sep=\";\", encoding = \"utf-8\")\n","  return pred_tags, valid_tags, valid_inputs\n","\n","def aplica_wv(x):\n","  try:\n","    res = word2vec_vectors[x]\n","  except:\n","    res = np.zeros(300)\n","  return res\n"]},{"cell_type":"markdown","metadata":{"id":"R2oHlmapz7Dk"},"source":["### Análisis"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":424526,"status":"aborted","timestamp":1663180682121,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"p90UG2axtG3D"},"outputs":[],"source":["corpus_anotacion = ([\"/content/corpora-master/2021/ref/training/wikinews.300.es.txt\", \"/content/corpora-master/2021/ref/training/wikinews.300.es.ann\"],\n","                    [\"/content/corpora-master/2021/ref/training/medline.1200.es.txt\", \"/content/corpora-master/2021/ref/training/medline.1200.es.ann\"]\n","                    )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":423984,"status":"aborted","timestamp":1663180682122,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"OFDFqjRAg7RI"},"outputs":[],"source":["raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","log_file = \"/content/drive/MyDrive/COLAB - TFM/full_training.log\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":423450,"status":"aborted","timestamp":1663180682122,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"D5q-HmqVhvae"},"outputs":[],"source":["corpus, anotacion = corpus_anotacion[0]\n","resultado = corpus.split(\"/\")[-1]\n","print(raiz+resultado+\"_analisis_oraciones.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":422813,"status":"aborted","timestamp":1663180682122,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"j_BuQgUjg8RN"},"outputs":[],"source":["for corpus, anotacion in corpus_anotacion:\n","  resultado = corpus.split(\"/\")[-1]\n","  ingles, espanol = clasifica_data(file=corpus)\n","  if len(ingles) \u003e 0:\n","    df_ingles_analiza, MAX_LEN = data_preprocessing_en(ingles, ann = anotacion)\n","    # Longitud de la oracion\n","    df_tmp = df_ingles_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","    df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_ingles_analiza, df_tmp[[\"Sentence #\", \"longitud\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    # Repeticion de las palabras en el corpus para calcular pesos\n","    df_tmp = df_ingles_analiza.groupby(\"Word\").POS.count().reset_index()\n","    df_tmp[\"rep_palabras\"] = df_tmp[\"POS\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Word\", \"rep_palabras\"]],\n","                                on=[\"Word\"], how=\"left\")\n","    df_cruce_ingles[\"palabra_ratio\"] = df_cruce_ingles[\"rep_palabras\"]/len(df_cruce_ingles)\n","    # Repeticion de las POS en el corpus para calcular pesos\n","    df_tmp = df_ingles_analiza.groupby(\"POS\").Word.count().reset_index()\n","    df_tmp[\"rep_pos\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"POS\", \"rep_pos\"]],\n","                                on=[\"POS\"], how=\"left\")\n","    df_cruce_ingles[\"pos_ratio\"] = df_cruce_ingles[\"rep_pos\"]/len(df_cruce_ingles)\n","    df_cruce_ingles[\"freq_palabra\"] = df_cruce_ingles[\"palabra_ratio\"] \u003c df_cruce_ingles[\"palabra_ratio\"].quantile(0.33)\n","    df_cruce_ingles[\"freq_pos\"] = df_cruce_ingles[\"pos_ratio\"] \u003c df_cruce_ingles[\"pos_ratio\"].quantile(0.33)\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_palabra.sum().reset_index()\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_palabra.sum().reset_index().sort_values(\"freq_palabra\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_palabra\", \"Sentence #\", \"freq_palabra\"]\n","    df_tmp[\"sum_freq_palabra\"] = df_tmp[\"freq_palabra\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"sum_freq_palabra\", \"rank_freq_palabra\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_pos.sum().reset_index().sort_values(\"freq_pos\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_pos\", \"Sentence #\", \"freq_pos\"]\n","    df_tmp[\"sum_freq_pos\"] = df_tmp[\"freq_pos\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"sum_freq_pos\", \"rank_freq_pos\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_ingles[\"wv\"] = df_cruce_ingles[\"Word\"].apply(aplica_wv)\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").wv.mean().apply(np.linalg.norm).reset_index()\n","    df_tmp[\"dif_wv_33\"] = df_tmp[\"wv\"] -  df_tmp[\"wv\"].median()\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"dif_wv_33\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_ingles[[\"Sentence #\", 'longitud', 'sum_freq_palabra', 'rank_freq_palabra', 'sum_freq_pos', 'rank_freq_pos', 'dif_wv_33']].drop_duplicates().to_csv(raiz+resultado+\"_analisis_oraciones_TEST.csv\")\n","    df_cruce_ingles[[\"Sentence #\",\t\"Word\",\t\"POS\",\t\"Tag\",\t\"longitud\"]].to_csv(raiz+resultado+\"_preprocessing_TEST.csv\")\n","  if len(espanol) \u003e 0:\n","    df_espanol_analiza, MAX_LEN = data_preprocessing_es(espanol, ann = anotacion)\n","    # Longitud de la oracion\n","    df_tmp = df_espanol_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","    df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_espanol_analiza, df_tmp[[\"Sentence #\", \"longitud\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    # Repeticion de las palabras en el corpus para calcular pesos\n","    df_tmp = df_espanol_analiza.groupby(\"Word\").POS.count().reset_index()\n","    df_tmp[\"rep_palabras\"] = df_tmp[\"POS\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Word\", \"rep_palabras\"]],\n","                                on=[\"Word\"], how=\"left\")\n","    df_cruce_espanol[\"palabra_ratio\"] = df_cruce_espanol[\"rep_palabras\"]/len(df_cruce_espanol)\n","    # Repeticion de las POS en el corpus para calcular pesos\n","    df_tmp = df_espanol_analiza.groupby(\"POS\").Word.count().reset_index()\n","    df_tmp[\"rep_pos\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"POS\", \"rep_pos\"]],\n","                                on=[\"POS\"], how=\"left\")\n","    df_cruce_espanol[\"pos_ratio\"] = df_cruce_espanol[\"rep_pos\"]/len(df_cruce_espanol)\n","    df_cruce_espanol[\"freq_palabra\"] = df_cruce_espanol[\"palabra_ratio\"] \u003c df_cruce_espanol[\"palabra_ratio\"].quantile(0.33)\n","    df_cruce_espanol[\"freq_pos\"] = df_cruce_espanol[\"pos_ratio\"] \u003c df_cruce_espanol[\"pos_ratio\"].quantile(0.33)\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_palabra.sum().reset_index()\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_palabra.sum().reset_index().sort_values(\"freq_palabra\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_palabra\", \"Sentence #\", \"freq_palabra\"]\n","    df_tmp[\"sum_freq_palabra\"] = df_tmp[\"freq_palabra\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"sum_freq_palabra\", \"rank_freq_palabra\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_pos.sum().reset_index().sort_values(\"freq_pos\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_pos\", \"Sentence #\", \"freq_pos\"]\n","    df_tmp[\"sum_freq_pos\"] = df_tmp[\"freq_pos\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"sum_freq_pos\", \"rank_freq_pos\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_espanol[\"wv\"] = df_cruce_espanol[\"Word\"].apply(aplica_wv)\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").wv.mean().apply(np.linalg.norm).reset_index()\n","    df_tmp[\"dif_wv_33\"] = df_tmp[\"wv\"] -  df_tmp[\"wv\"].median()\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"dif_wv_33\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_espanol[[\"Sentence #\", 'longitud', 'sum_freq_palabra', 'rank_freq_palabra', 'sum_freq_pos', 'rank_freq_pos', 'dif_wv_33']].drop_duplicates().to_csv(raiz+resultado+\"_analisis_oraciones_TEST.csv\")\n","    df_cruce_espanol[[\"Sentence #\",\t\"Word\",\t\"POS\",\t\"Tag\",\t\"longitud\"]].to_csv(raiz+resultado+\"_preprocessing_TEST.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100507,"status":"aborted","timestamp":1663180224782,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"Y6tDSWW6hzYN"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":100506,"status":"aborted","timestamp":1663180224782,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"},"user_tz":-120},"id":"BgJD63uejDbN"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOtkut+Kq8V7VE0l4BrR/2C","collapsed_sections":["pqugLk_Lzyms"],"name":"","version":""},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}