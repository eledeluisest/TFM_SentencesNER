{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyUhFcfeOHUK1wRR2XDhyO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Instalaciones\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","!pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ny8r40vLIT7E","executionInfo":{"status":"ok","timestamp":1663204652554,"user_tz":-120,"elapsed":27789,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"4604bd77-717a-4761-9954-fe08da84d8ea"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Collecting boto3\n","  Downloading boto3-1.24.73-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 25.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.9 MB/s \n","\u001b[?25hCollecting botocore<1.28.0,>=1.27.73\n","  Downloading botocore-1.27.73-py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 67.8 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 73.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.73->boto3->pytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.73->boto3->pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.24.73 botocore-1.27.73 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 51 kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=dc630d0b6599eb88fbd08c892a9b890c7749560748d2382e3d90644136f4a92f\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=4123c806cca4b5905100e5eba6e3b7f4ccca667e57d7b2ab6b2446f70a0c080d\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}]},{"cell_type":"code","source":["# Importaciones\n","# GENERAL Y PREPROCESADO\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","import spacy\n","!python -m spacy download es_core_news_sm\n","import es_core_news_sm\n","from langdetect import detect\n","import os.path\n","import datetime\n","# BERT\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","from seqeval.metrics import f1_score\n","\n","\n","from IPython.display import clear_output "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N0lhb4BaIZJz","executionInfo":{"status":"ok","timestamp":1663204679020,"user_tz":-120,"elapsed":26473,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"2c3e8fb8-f2a9-4757-b8e8-497f7e44f3d9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 4.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}]},{"cell_type":"code","source":["# Conexion a drive y descompresión de los corpus\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z5sgfZftIfMI","executionInfo":{"status":"ok","timestamp":1663204822012,"user_tz":-120,"elapsed":143003,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"a788e721-5c64-4a41-800c-0c97259a4d7b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# funciones\n"],"metadata":{"id":"NEL_SG2stPGt"}},{"cell_type":"code","source":["def clasifica_data(file):\n","  file_csv = file\n","  ingles = []\n","  espanol = []\n","  with open(file_csv, \"r\") as f:\n","    texto = f.read().split(\"\\n\")\n","  for frase in texto:\n","    espaciado = frase.split(\" \")\n","    idioma = detect(frase)\n","    if idioma == \"en\":\n","      ingles.append(frase)\n","    elif idioma == \"es\":\n","      espanol.append(frase)\n","  return ingles, espanol\n","\n","  \n","def data_preprocessing_es(texto, ann = None):\n","  nlp = es_core_news_sm.load()\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","def data_preprocessing_en(texto, ann = None):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","def bert_formating(data, MAX_LEN, test_size=0.2, bs = 16):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                              random_state=2022, test_size=test_size)\n","  tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                              random_state=2022, test_size=test_size)\n","  tr_inputs = torch.tensor(tr_inputs)\n","  val_inputs = torch.tensor(val_inputs)\n","  tr_tags = torch.tensor(tr_tags)\n","  val_tags = torch.tensor(val_tags)\n","  tr_masks = torch.tensor(tr_masks)\n","  val_masks = torch.tensor(val_masks)\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, valid_data, valid_sampler,\\\n","            valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs\n","\n","def bert_formating_one_dataset(data, MAX_LEN, bs = 16, tag2idx = None, tags_vals=None,word2idx=None):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  if tags_vals is None:\n","    tags_vals = list(set(data[\"Tag\"].values))\n","  if tag2idx is None:\n","    tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words)\n","  if word2idx is None:\n","    word2idx = {w: i + 2 for i, w in enumerate(words)}\n","    word2idx[\"UNK\"] = 1\n","    word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  # tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","\n","  tokens_ids = []\n","  for s in tokenized_texts:\n","    tmp = []\n","    for w in s:\n","      try:\n","        tmp.append(word2idx[w])\n","      except:\n","        tmp.append(word2idx[\"UNK\"])\n","    tokens_ids.append(tmp)\n","  print(tokens_ids)\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs = torch.tensor(input_ids, dtype=torch.int64)\n","  tr_tags = torch.tensor(tags, dtype=torch.int64)\n","  tr_masks = torch.tensor(attention_masks, dtype = torch.uint8)\n","\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, tag2idx, device, tags_vals, idx2word, tr_inputs, word2idx\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = \"\"):\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()\n","  if reentreno:\n","    print(\"Loading existing model...\")\n","    model.load_state_dict(torch.load(save_path))\n","  if FULL_FINETUNING:\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'gamma', 'beta']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.01},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.0}\n","      ]\n","  else:\n","      param_optimizer = list(model.classifier.named_parameters()) \n","      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","  return model, optimizer\n","\n","def training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = 200,\n","             max_grad_norm = 1.0, save_path = \"./bert1\", early_stopping=15, use_scheduler=True):\n","  lmbda = lambda x: 0.98 if use_scheduler else 1\n","  scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","  train_f1 = []\n","  train_losses = []\n","  val_losses = []\n","  contador_max = -1\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","      # TRAIN loop\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          # add batch to gpu\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          # forward pass\n","          loss = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask, labels=b_labels)\n","          # backward pass\n","          loss.backward()\n","          # track train loss\n","          tr_loss += loss.item()\n","          nb_tr_examples += b_input_ids.size(0)\n","          nb_tr_steps += 1\n","          # gradient clipping\n","          # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","          # update parameters\n","          optimizer.step()\n","          model.zero_grad()\n","      scheduler.step()\n","      # print train loss per epoch\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","      # VALIDATION on validation set\n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels, true_inputs = [], [],[]\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch          \n","          with torch.no_grad():\n","              tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                    attention_mask=b_input_mask, labels=b_labels)\n","              logits = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          inputs = b_input_ids.to('cpu').numpy()\n","          \n","          true_inputs.append(inputs)\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.append(label_ids)\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          eval_loss += tmp_eval_loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","          \n","          nb_eval_examples += b_input_ids.size(0)\n","          nb_eval_steps += 1\n","      eval_loss = eval_loss/nb_eval_steps\n","      print(\"Validation loss: {}\".format(eval_loss))\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","      valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","      valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","      f1 = f1_score([pred_tags], [valid_tags])\n","      train_f1.append(f1)\n","      train_losses.append(tr_loss/nb_tr_steps)\n","      val_losses.append(eval_loss)\n","\n","      max_f1 = max(train_f1)\n","      if f1 == max_f1:\n","        contador_max = 1\n","        torch.save(model.state_dict(), save_path)\n","      if contador_max > 0:\n","        contador_max += 1\n","      print(\"F1-Score: \" + str(train_f1[-1]))\n","      if round(max_f1, 2) > 0.1 and contador_max > early_stopping :\n","        print(\"Early stopping...\")\n","        return 0\n","def training_scheduler(model, optimizer, loss_fn, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs=200,\n","             max_grad_norm=1.0, save_path=\"./bert1\", early_stopping=15, print_freq=10, use_scheduler=True):\n","    lmbda = lambda x: 0.98 if use_scheduler else 1\n","    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n","\n","    train_f1 = []\n","    train_losses = []\n","    val_losses = []\n","    contador_max = -1\n","\n","    for ep in range(epochs):\n","        # TRAIN loop\n","        model.train()\n","        tr_loss = 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        for step, batch in enumerate(train_dataloader):\n","            # add batch to gpu\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            # forward pass\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","            loss = model.compute_loss(loss_fn, logits, b_labels, b_input_mask)\n","            # backward pass\n","            loss.backward()\n","            # track train loss\n","            tr_loss += loss.item()\n","            nb_tr_examples += b_input_ids.size(0)\n","            nb_tr_steps += 1\n","            # gradient clipping\n","            # torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","            # update parameters\n","            optimizer.step()\n","            model.zero_grad()\n","        scheduler.step()\n","        # print train loss per epoch\n","        if ep % print_freq == 0:\n","            print(f\"{ep}/{epochs}: Train loss: {tr_loss / nb_tr_steps}\")\n","\n","        # VALIDATION on validation set\n","        model.eval()\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_steps, nb_eval_examples = 0, 0\n","        predictions, true_labels, true_inputs = [], [], []\n","        for batch in valid_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","            with torch.no_grad():\n","                logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","\n","            tmp_eval_loss = model.compute_loss(loss_fn, logits, b_labels, b_input_mask)\n","\n","            logits = logits.cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","            inputs = b_input_ids.to('cpu').numpy()\n","\n","            true_inputs.append(inputs)\n","            predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","            true_labels.append(label_ids)\n","\n","            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","            eval_loss += tmp_eval_loss.item()\n","            eval_accuracy += tmp_eval_accuracy\n","\n","            nb_eval_examples += b_input_ids.size(0)\n","            nb_eval_steps += 1\n","\n","        eval_loss = eval_loss / nb_eval_steps\n","        if ep % print_freq == 0:\n","            print(\"\\tValidation loss: {}\".format(eval_loss))\n","            print(\"\\tValidation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))\n","        pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","        valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","        valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in true_inputs for l_i in l]\n","        f1 = f1_score([pred_tags], [valid_tags])\n","        train_f1.append(f1)\n","        train_losses.append(tr_loss / nb_tr_steps)\n","        val_losses.append(eval_loss)\n","\n","        max_f1 = max(train_f1)\n","        if f1 == max_f1:\n","            contador_max = 1\n","            torch.save(model.state_dict(), save_path)\n","        if contador_max > 0:\n","            contador_max += 1\n","        if ep % print_freq == 0:\n","            print(f\"\\tF1-Score: {train_f1[-1]}\\n\")\n","\n","        if round(max_f1, 2) > 0.1 and contador_max > early_stopping:\n","            print(\"We should apply early stopping now\")\n","\n","def evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = \"./bert1\",\n","             guarda_resultado=\"/content/drive/MyDrive/COLAB - TFM/resultado_entrenamiento1.csv\"):\n","  #evaluate model\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  model.load_state_dict(torch.load(save_path))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()  \n","  model.eval()\n","  predictions = []\n","  true_labels = []\n","  true_inputs = []\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","  print(len(valid_dataloader))\n","  for batch in tqdm(valid_dataloader):\n","      #print(len(batch))\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","\n","      with torch.no_grad():\n","          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                attention_mask=b_input_mask, labels=b_labels)\n","          logits = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","          \n","      logits = logits.detach().cpu().numpy()\n","      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      label_ids = b_labels.to('cpu').numpy()\n","      inputs = b_input_ids.to('cpu').numpy()\n","      true_inputs.append(inputs)\n","      \n","      \n","      true_labels.append(label_ids)\n","      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","\n","  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n","  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n","  valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","\n","  count = 0 \n","  all_data = 0\n","  registro_resultado = []\n","  for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n","      for k,l,kk in zip(i,j,ll):\n","        count += 1\n","        registro_resultado.append([k,l,idx2word[kk.item()]])\n","        all_data += 1\n","  df_alldata = pd.DataFrame(registro_resultado)\n","  df_alldata.columns = [\"pred_tags\", \"valid_tags\", \"val_inputs\"]\n","  df_alldata[df_alldata[\"val_inputs\"] != \"PAD\"].\\\n","        to_csv(guarda_resultado,sep=\";\", encoding = \"utf-8\")\n","  return pred_tags, valid_tags, valid_inputs"],"metadata":{"id":"A7D2VuBVIf4u","executionInfo":{"status":"ok","timestamp":1663204822419,"user_tz":-120,"elapsed":415,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# No funciones"],"metadata":{"id":"bmfZgm8YtSsc"}},{"cell_type":"code","source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from math import floor, ceil\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"fuya4fhOIt9o","executionInfo":{"status":"ok","timestamp":1663204822420,"user_tz":-120,"elapsed":4,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["\n","raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","tag2idx = {'B-Reference': 0, 'I-Concept': 1, 'O': 2, 'I-Predicate': 3, 'B-Action': 4, 'B-Predicate': 5, 'B-Concept': 6, 'I-Action': 7}\n","tags_vals = ['B-Reference', 'I-Concept', 'O', 'I-Predicate', 'B-Action', 'B-Predicate', 'B-Concept', 'I-Action']\n","device = \"cuda\"\n","modelo_ES25 = raiz + \"1200_bert_es25_sum_freq_pos_iterMODELO_FINAL\"\n","model_ES25, optimizer_ES50 = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= True, save_path = modelo_ES25)\n","modelo_ES50 = raiz + \"1200_bert_es50_sum_freq_pos_iterMODELO_FINAL\"\n","model_ES50, optimizer_ES50 = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= True, save_path = modelo_ES50)\n","modelo_EN25 = raiz + \"1200_bert_en25_sum_freq_pos_iterMODELO_FINAL\"\n","model_EN25, optimizer_EN25 = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= True, save_path = modelo_EN25)\n","modelo_EN50 = raiz + \"1200_bert_en50_sum_freq_pos_iterMODELO_FINAL\"\n","model_EN50, optimizer_EN50 = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= True, save_path = modelo_EN50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H0d3YT59Iz1U","executionInfo":{"status":"ok","timestamp":1663204909106,"user_tz":-120,"elapsed":86690,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"196e80da-d4de-4893-81c5-b16845b6cd72"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 407873900/407873900 [00:27<00:00, 14631727.42B/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading existing model...\n","Loading existing model...\n","Loading existing model...\n","Loading existing model...\n"]}]},{"cell_type":"code","source":["num_reg = [1200]\n","raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","df_cord = pd.read_csv(raiz+\"cord_resumen_oraciones.csv\", index_col=0)\n","df_wikinews = pd.read_csv(raiz+\"wikinews_resumen_oraciones.csv\", index_col=0)\n","df_medline = pd.read_csv(raiz+\"medline_resumen_oraciones.csv\", index_col=0)\n","\n","# seleccionamos los archivos que conforman el conjunto de test"],"metadata":{"id":"RlQNX2Ubtz0Q","executionInfo":{"status":"ok","timestamp":1663204911279,"user_tz":-120,"elapsed":2183,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["num_reg = [1200]\n","reg = 1200\n","raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","df_cord = pd.read_csv(raiz+\"cord_resumen_oraciones.csv\", index_col=0)\n","df_wikinews = pd.read_csv(raiz+\"wikinews_resumen_oraciones.csv\", index_col=0)\n","df_medline = pd.read_csv(raiz+\"medline_resumen_oraciones.csv\", index_col=0)\n"],"metadata":{"id":"O7qfKnp-IvQ3","executionInfo":{"status":"ok","timestamp":1663205695696,"user_tz":-120,"elapsed":381,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["corpus_anotacion = ([\"/content/corpora-master/2021/ref/training/wikinews.300.es.txt\", \"/content/corpora-master/2021/ref/training/wikinews.300.es.ann\"],\n","                    [\"/content/corpora-master/2021/ref/training/medline.1200.es.txt\", \"/content/corpora-master/2021/ref/training/medline.1200.es.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/cord.150.txt\", \"/content/corpora-master/2021/ref/testing/cord.150.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/medline.75.txt\", \"/content/corpora-master/2021/ref/testing/medline.75.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/wikinews.75.txt\", \"/content/corpora-master/2021/ref/testing/wikinews.75.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/cord.50.txt\", \"/content/corpora-master/2021/ref/develop/cord.50.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/medline.25.txt\", \"/content/corpora-master/2021/ref/develop/medline.25.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/wikinews.25.txt\", \"/content/corpora-master/2021/ref/develop/wikinews.25.ann\"],\n","                    )"],"metadata":{"id":"J1YsBtodu4ty"}},{"cell_type":"code","source":["df_cord_dev = df_cord.loc[df_cord[\"fichero\"] == 150,:]\n","df_cord_dev.loc[:, \"nombre_fichero\"] = \"cord\"\n","df_wikinews_dev = df_wikinews.loc[df_wikinews[\"fichero\"] == 75,:]\n","df_wikinews_dev.loc[:, \"nombre_fichero\"] = \"wikinews\"\n","df_medline_dev = df_medline.loc[df_medline[\"fichero\"] == 75,:]\n","df_medline_dev.loc[:, \"nombre_fichero\"] = \"medline\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NBN8tiVUu6TN","executionInfo":{"status":"ok","timestamp":1663206500520,"user_tz":-120,"elapsed":385,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"455ad0cc-1fc4-4197-f9cf-c1c43abe54ac"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self.obj[key] = value\n"]}]},{"cell_type":"code","source":["df_dev = pd.concat([df_cord_dev, df_wikinews_dev, df_medline_dev], axis=0).reset_index(drop=True)\n","df_train = pd.read_csv(raiz+str(reg)+\"_sum_freq_pos.csv\")"],"metadata":{"id":"EpeiIjgLwHCv","executionInfo":{"status":"ok","timestamp":1663206501435,"user_tz":-120,"elapsed":3,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["df_train.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"o0LRF0yawbOF","executionInfo":{"status":"ok","timestamp":1663206502036,"user_tz":-120,"elapsed":6,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"a773b46a-f9d1-4d30-f7c8-4dcee310d3f8"},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Unnamed: 0  Sentence #  fichero nombre_fichero\n","0           0          17       50           cord\n","1           1          29       50           cord\n","2           2          10       50           cord\n","3           3          44       50           cord\n","4           4           0       50           cord"],"text/html":["\n","  <div id=\"df-0999a70b-6dd8-4766-918f-4b4149a0a58c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Sentence #</th>\n","      <th>fichero</th>\n","      <th>nombre_fichero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>17</td>\n","      <td>50</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>50</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>10</td>\n","      <td>50</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>44</td>\n","      <td>50</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>cord</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0999a70b-6dd8-4766-918f-4b4149a0a58c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0999a70b-6dd8-4766-918f-4b4149a0a58c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0999a70b-6dd8-4766-918f-4b4149a0a58c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":[],"metadata":{"id":"wJDrpcFqwsOX","executionInfo":{"status":"ok","timestamp":1663206502037,"user_tz":-120,"elapsed":5,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["df_dev.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Js5HK_4IxxqA","executionInfo":{"status":"ok","timestamp":1663206503804,"user_tz":-120,"elapsed":5,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"3e44754e-75fe-49a7-f8fe-5864db20b678"},"execution_count":57,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Sentence #  longitud  sum_freq_palabra  rank_freq_palabra  sum_freq_pos  \\\n","0           0        27                 2                126             6   \n","1           1        21                 5                 66             2   \n","2           2        23                 5                 71             5   \n","3           3        21                 2                114             5   \n","4           4        23                 4                 96             4   \n","\n","   rank_freq_pos  dif_wv_33  fichero nombre_fichero  \n","0             72   0.085513      150           cord  \n","1            138   0.105187      150           cord  \n","2             98  -0.099989      150           cord  \n","3             87  -0.146534      150           cord  \n","4            102   0.049033      150           cord  "],"text/html":["\n","  <div id=\"df-702cdf68-60df-4d2d-8b51-e504c411272e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>longitud</th>\n","      <th>sum_freq_palabra</th>\n","      <th>rank_freq_palabra</th>\n","      <th>sum_freq_pos</th>\n","      <th>rank_freq_pos</th>\n","      <th>dif_wv_33</th>\n","      <th>fichero</th>\n","      <th>nombre_fichero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>27</td>\n","      <td>2</td>\n","      <td>126</td>\n","      <td>6</td>\n","      <td>72</td>\n","      <td>0.085513</td>\n","      <td>150</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>5</td>\n","      <td>66</td>\n","      <td>2</td>\n","      <td>138</td>\n","      <td>0.105187</td>\n","      <td>150</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>23</td>\n","      <td>5</td>\n","      <td>71</td>\n","      <td>5</td>\n","      <td>98</td>\n","      <td>-0.099989</td>\n","      <td>150</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>21</td>\n","      <td>2</td>\n","      <td>114</td>\n","      <td>5</td>\n","      <td>87</td>\n","      <td>-0.146534</td>\n","      <td>150</td>\n","      <td>cord</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>23</td>\n","      <td>4</td>\n","      <td>96</td>\n","      <td>4</td>\n","      <td>102</td>\n","      <td>0.049033</td>\n","      <td>150</td>\n","      <td>cord</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-702cdf68-60df-4d2d-8b51-e504c411272e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-702cdf68-60df-4d2d-8b51-e504c411272e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-702cdf68-60df-4d2d-8b51-e504c411272e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tfq0U9U8xPbg","executionInfo":{"status":"ok","timestamp":1663206504687,"user_tz":-120,"elapsed":5,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"06a69f86-baef-438f-c675-71dfce80826e"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["6122\n","2398\n"]}]},{"cell_type":"code","source":["modelo_ES25"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"wK0eTHmA1uFQ","executionInfo":{"status":"ok","timestamp":1663206616982,"user_tz":-120,"elapsed":28,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"3da06521-ee4a-4fa1-a853-e78279ca158b"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/COLAB - TFM/1200_bert_es25_sum_freq_pos_iterMODELO_FINAL'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["modelo_ES25 = raiz + \"1200_bert_es25_sum_freq_pos_iterMODELO_FINAL\"\n","modelo_ES50 = raiz + \"1200_bert_es50_sum_freq_pos_iterMODELO_FINAL\"\n","modelo_EN25 = raiz + \"1200_bert_en25_sum_freq_pos_iterMODELO_FINAL\"\n","modelo_EN50 = raiz + \"1200_bert_en50_sum_freq_pos_iterMODELO_FINAL\""],"metadata":{"id":"PhIz2kTL3n24"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["modelo_ES25 = raiz + \"120000_bert_es25_sum_freq_pos_iterBASELINE\"\n","modelo_ES50 = raiz + \"120000_bert_es50_sum_freq_pos_iterBASELINE\"\n","modelo_EN25 = raiz + \"120000_bert_en25_sum_freq_pos_iterBASELINE\"\n","modelo_EN50 = raiz + \"120000_bert_en50_sum_freq_pos_iterBASELINE\""],"metadata":{"id":"uBlAGic23qgE","executionInfo":{"status":"ok","timestamp":1663207349030,"user_tz":-120,"elapsed":376,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# !! cambiar\n","reg = 120000\n","df_cord_dev = df_cord.loc[df_cord[\"fichero\"] == 150,:]\n","df_cord_dev.loc[:, \"nombre_fichero\"] = \"cord\"\n","df_wikinews_dev = df_wikinews.loc[df_wikinews[\"fichero\"] == 75,:]\n","df_wikinews_dev.loc[:, \"nombre_fichero\"] = \"wikinews\"\n","df_medline_dev = df_medline.loc[df_medline[\"fichero\"] == 75,:]\n","df_medline_dev.loc[:, \"nombre_fichero\"] = \"medline\"\n","df_dev = pd.concat([df_cord_dev, df_wikinews_dev, df_medline_dev], axis=0).reset_index(drop=True)\n","df_train = pd.read_csv(raiz+str(reg)+\"_sum_freq_pos.csv\")\n","sufijo = \"_preprocessing.csv\"\n","archivos = [\"wikinews.25.txt\",\"wikinews.75.txt\",\"wikinews.300.es.txt\",\n","            \"medline.25.txt\",\"medline.75.txt\",\"medline.1200.es.txt\",\n","            \"cord.50.txt\",\"cord.150.txt\"]\n","for i, archivo in enumerate(archivos):\n","  if i == 0:\n","    df = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","    df[\"fichero\"] = archivo.split(\".\")[1]\n","    df[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","  else:\n","    df_tmp = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","    df_tmp[\"fichero\"] = archivo.split(\".\")[1]\n","    df_tmp[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","    df = pd.concat([df,df_tmp], axis=0).reset_index(drop=True)\n","# !! cambiar\n","lim_i = 25\n","lim_f = 50\n","df_dev_seleccion =df_dev\n","df[\"fichero\"] = df[\"fichero\"].astype(int)\n","df_dev = pd.merge(df, df_dev_seleccion[[\"Sentence #\", \"fichero\", \"nombre_fichero\"]], on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","print(len(df_dev))\n","df_dev = df_dev.loc[(df_dev[\"longitud\"] > lim_i) & (df_dev[\"longitud\"] <= lim_f), :].copy()\n","df_dev[\"Sentence #\"] = df_dev[\"Sentence #\"].astype(str) + \"_\" + df_dev[\"fichero\"].astype(str)  + \"_\" + df_dev[\"nombre_fichero\"]\n","print(len(df_dev))\n","df_ingles_dev = df_dev.loc[df_dev[\"nombre_fichero\"] == \"cord\"]\n","df_espanol_dev = df_dev.loc[df_dev[\"nombre_fichero\"] != \"cord\"]\n","df_seleccion = df_train\n","df[\"fichero\"] = df[\"fichero\"].astype(int)\n","df_train = pd.merge(df, df_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","print(len(df_train))\n","df_train = df_train.loc[(df_train[\"longitud\"] > lim_i) & (df_train[\"longitud\"] <= lim_f), :].copy()\n","df_train[\"Sentence #\"] = df_train[\"Sentence #\"].astype(str) + \"_\" + df_train[\"fichero\"].astype(str) + \"_\" + df_train[\"nombre_fichero\"]\n","print(len(df_train))\n","df_ingles_analiza = df_train.loc[df_train[\"nombre_fichero\"] == \"cord\"]\n","print(len(df_ingles_analiza))\n","df_espanol_analiza = df_train.loc[df_train[\"nombre_fichero\"] != \"cord\"]\n","print(len(df_espanol_analiza))\n","# !! cambiar\n","data = df_espanol_analiza.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","# data = df_ingles_analiza.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","tag2idx = {'B-Reference': 0, 'I-Concept': 1, 'O': 2, 'I-Predicate': 3, 'B-Action': 4, 'B-Predicate': 5, 'B-Concept': 6, 'I-Action': 7}\n","tags_vals = ['B-Reference', 'I-Concept', 'O', 'I-Predicate', 'B-Action', 'B-Predicate', 'B-Concept', 'I-Action']\n","MAX_LEN = lim_f\n","BS = 16\n","train_data, train_sampler, train_dataloader, tag2idx_tr, device, tags_vals_tr, idx2word_tr, tr_inputs, word2idx = bert_formating_one_dataset(data, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS)\n","# !! cambiar\n","data_dev = df_espanol_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","# data_dev = df_ingles_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","# !! cambiar\n","modelo = modelo_ES50\n","resultado = \"baseline_es50.csv\"\n","\n","BS = len(data_dev)\n","valid_data, valid_sampler, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, word2idx = bert_formating_one_dataset(data_dev, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS,word2idx=word2idx)\n","pred_tags, valid_tags, valid_inputs = evaluate(modelo, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n","            guarda_resultado=raiz+resultado)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zBWHCVONwH_l","executionInfo":{"status":"ok","timestamp":1663207509943,"user_tz":-120,"elapsed":16500,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"0ac221fc-069f-4501-ac2e-290f6e20ef3b"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1667: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  self.obj[key] = value\n"]},{"output_type":"stream","name":"stdout","text":["6122\n","2398\n","23140\n","5377\n","603\n","4774\n","[[1318, 1283, 689, 1386, 1318, 204, 877, 778, 166, 1154, 190, 705, 1158, 1419, 914, 246, 400, 1318, 101, 627, 877, 945, 1246, 897, 628, 166, 358, 987, 1349, 1386, 897, 274], [897, 547, 705, 340, 166, 320, 1318, 163, 665, 275, 1318, 245, 705, 812, 1065, 1028, 166, 1048, 772, 849, 868, 358, 325, 1303, 299, 166, 972], [1030, 1210, 1424, 839, 550, 1318, 80, 1424, 407, 1442, 87, 166, 897, 1329, 1039, 275, 897, 1279, 166, 294, 877, 1450, 1065, 560, 330, 275, 1412, 1036, 1417, 1419, 406, 122, 1022, 592, 166, 364, 166, 1032, 275, 57], [1450, 1319, 1454, 1318, 252, 1145, 320, 854, 613, 1018, 764, 735, 74, 1438, 1054, 229, 877, 358, 1217, 1424, 676, 651, 1386, 585, 897, 461, 877, 204, 275, 897, 1027, 1374, 1116], [897, 1436, 1018, 166, 149, 891, 1424, 716, 911, 383, 897, 224, 985, 362, 1145, 1318, 1226, 1386, 667, 275, 897, 257, 1103, 1374, 149, 317], [788, 897, 671, 166, 358, 438, 967, 275, 1001, 1175, 166, 1122, 1318, 870, 1052, 166, 804, 1245, 336, 566, 166, 897, 633, 656, 940, 505, 985, 62, 897, 334, 166, 743, 1087, 218, 1035, 205, 633, 437, 1318, 557, 745, 140, 218, 1318, 334, 502, 625, 881], [275, 1318, 1455, 238, 393, 1212, 1178, 552, 166, 790, 275, 205, 1055, 166, 897, 662, 1040, 469, 669, 320, 147, 1419, 73, 166, 941, 1318, 1269, 460, 275, 1318, 774, 220, 1318, 568, 390, 1075, 1426, 1318, 1455, 1256, 437, 1065, 196, 135, 897, 1299], [973, 715, 269, 806, 1108, 166, 215, 1185, 821, 877, 728, 500, 897, 1013, 166, 1001, 1195, 782, 31, 1050, 343, 164, 1328, 106, 1166, 754, 1362, 15, 297], [218, 711, 320, 1171, 166, 692, 877, 784, 1001, 799, 180, 939, 1318, 204, 166, 1127, 800, 166, 41, 983, 166, 1421, 509, 491, 1071, 116, 919, 166, 1237, 1109, 639, 166, 612, 877, 369, 572, 166, 897, 796, 766, 166, 835], [358, 268, 982, 1424, 1146, 853, 218, 897, 1304, 1434, 320, 1318, 1358, 890, 1050, 253, 764, 684, 320, 47, 1159, 166, 897, 339, 553, 166, 1219, 877, 320, 193, 166, 1249, 1054, 756], [1386, 1318, 3, 166, 422, 897, 456, 877, 946, 1318, 694, 166, 897, 1190, 275, 1318, 11, 166, 1180, 178, 877, 275, 1315, 437, 1001, 1020, 107, 1298, 1035, 478, 1380, 275, 538, 1430], [506, 1025, 1055, 166, 1065, 1268, 166, 603, 320, 83, 437, 470, 1419, 999, 166, 1029, 458, 79, 166, 1035, 1184, 166, 1174, 877, 909, 320, 1261, 1419, 410, 166, 358, 867, 1332], [897, 1127, 166, 114, 183, 1199, 437, 1035, 167, 1331, 658, 218, 897, 430, 308, 1386, 897, 1005, 166, 404, 906, 1419, 897, 871, 166, 1001, 752, 320, 275, 1001, 200, 17, 1143, 897, 511, 166, 25, 1170, 877, 764, 240, 882, 320, 297, 1303, 857, 676, 593], [897, 865, 166, 444, 1424, 705, 1445, 218, 1140, 166, 1456, 570, 877, 30, 1020, 202, 320, 271, 1359, 420, 1442, 1354, 232, 1131, 1050, 1130, 437, 897, 980, 166, 528, 594, 275, 1398, 1374, 962, 870], [1318, 847, 1215, 139, 166, 320, 1318, 1213, 929, 384, 471, 1031, 1065, 101, 694, 166, 652, 275, 12, 1419, 897, 27, 166, 897, 1111, 166, 781, 218, 1055, 166, 309], [1001, 293, 1054, 1229, 1428, 881, 121, 1094, 897, 286, 166, 207, 557, 166, 320, 493, 1113, 1318, 334, 166, 897, 633, 1419, 1368, 166, 320, 207, 557, 121, 1401, 1220, 437, 251, 659, 1093, 166, 897, 633, 656, 940, 505, 320, 1100, 1300, 659], [897, 392, 166, 642, 54, 986, 289, 873, 1265, 124, 1440, 1318, 1323, 798, 587, 218, 358, 755, 166, 700, 922, 175, 166, 1410, 1277, 845, 166, 244, 877, 194, 275, 432, 437, 207, 644, 166, 1452, 1209, 523, 877, 1306, 74, 1374, 694], [1318, 1289, 509, 166, 897, 547, 35, 477, 713, 1168, 437, 1318, 1213, 166, 1191, 604, 1351, 645, 897, 1045, 166, 501, 512, 275, 1318, 1235, 166, 1214, 897, 1140, 1284, 218, 1001, 752, 1374, 870, 376, 275, 897, 1127, 166, 1191], [772, 328, 877, 275, 1035, 32, 1140, 770, 1318, 376, 1424, 1353, 320, 897, 619, 1051, 1374, 118, 166, 916, 166, 358, 1372, 166, 897, 769, 542, 1156, 1265, 537, 218, 441, 275, 285, 166, 181, 218, 1318, 1221, 320, 1006, 897, 662, 619, 1419, 897, 1409], [275, 324, 166, 933, 145, 463, 858, 863, 166, 897, 811, 509, 166, 897, 1190, 403, 320, 1318, 694, 575, 852, 816, 886, 1386, 706, 1419, 358, 156, 877, 1138, 1419, 358, 1425], [275, 485, 1137, 166, 715, 913, 166, 173, 201, 269, 7, 1140, 1035, 860, 166, 1090, 316, 920, 166, 897, 1127, 166, 1362, 705, 1310, 266], [649, 1419, 897, 132, 166, 591, 168, 1435, 320, 1318, 1074, 59, 945, 296, 1386, 1301, 988, 877, 320, 1113, 64, 1424, 72, 559, 218, 1001, 270, 166, 297, 857], [1167, 897, 811, 509, 166, 1231, 1374, 344, 166, 1191, 318, 1001, 635, 166, 1001, 200, 504, 857, 320, 703, 1419, 387, 40, 775, 877, 485, 1137, 705, 887, 635], [1318, 1213, 1381, 29, 1427, 545, 823, 37, 1424, 1418, 1030, 614, 869, 166, 872, 437, 358, 1115, 166, 358, 495, 166, 419, 494, 877, 728, 275, 1127, 1441, 1419, 1334, 1114, 166, 1252, 275, 494], [1424, 945, 434, 1318, 445, 166, 233, 166, 825, 166, 857, 503, 320, 59, 1424, 981, 704, 166, 63, 1035, 1011, 320, 805, 1424, 705, 584, 1386, 1318, 1116, 1065, 905, 10, 166, 63], [772, 328, 1303, 867, 139, 715, 1208, 353, 64, 981, 1196, 166, 144, 275, 897, 1406, 320, 803, 1134, 275, 1318, 1235, 429, 166, 1297, 182], [1318, 79, 1374, 357, 705, 217, 320, 897, 324, 1424, 261, 275, 504, 1356, 166, 173, 320, 284, 1345, 1050, 897, 1316, 1444, 877, 897, 415, 1374, 1142, 897, 668, 877, 897, 889, 1018, 897, 668, 275, 1001, 923, 119, 877, 897, 668, 1440, 897, 924, 563], [1125, 1419, 1368, 166, 1001, 248, 320, 711, 1204, 23, 897, 1169, 166, 358, 663, 166, 207, 557, 443, 605, 964, 1157, 218, 897, 1231, 275, 358, 818, 166, 283, 877, 1318, 894, 956, 115, 1419, 358, 664, 320, 1424, 729, 460, 900, 320, 1001, 314, 833], [275, 1318, 1220, 238, 877, 965, 1364, 1318, 345, 877, 1419, 358, 747, 915, 1065, 901, 218, 897, 561, 1265, 698, 218, 809, 1386, 1293, 207, 790, 877, 1318, 943, 1374, 557], [910, 515, 120, 1424, 716, 275, 87, 275, 897, 425, 213, 166, 1231, 1164, 1424, 102, 1386, 207, 87, 1065, 482, 166, 1189, 109, 166, 320, 1318, 694, 657, 320, 358, 127, 166, 897, 1223, 15, 1318, 344, 877, 897, 619, 22, 1024, 166, 344], [1442, 602, 1318, 464, 166, 1001, 250, 877, 1001, 355, 249, 544, 1318, 396, 1374, 725, 1284, 596, 445, 166, 718, 754, 1104, 1374, 725, 877, 1234], [302, 320, 1007, 320, 1001, 25, 1432, 1313, 1291, 1442, 317, 166, 1383, 793, 882, 320, 1318, 1405, 166, 897, 1361, 166, 288, 1424, 1151, 275, 1318, 310, 437, 1318, 20, 166, 1001, 1251, 166, 897, 360], [1318, 399, 877, 88, 199, 588, 773, 917, 70, 554, 1419, 358, 354, 867, 906, 1419, 1065, 77, 899, 320, 1063, 867, 1330, 1167, 1353, 1318, 1033, 971, 897, 1429, 275, 1318, 320, 1265, 846, 166, 524, 877, 1259], [275, 219, 1129, 166, 817, 289, 1435, 320, 897, 169, 1424, 1077, 218, 1255, 1315, 772, 328, 1180, 133, 64, 388, 437, 977, 517, 320, 1318, 694, 1082, 1419, 897, 619, 1131, 216, 166, 1295, 1227, 320, 64, 108, 1202, 1035, 759], [1318, 1459, 337, 166, 688, 1424, 519, 1035, 161, 275, 897, 379, 1164, 36, 722, 275, 160, 166, 451, 877, 1366, 1091, 320, 1424, 426, 897, 562, 166, 170], [50, 375, 275, 1318, 277, 1318, 1213, 166, 419, 998, 1336, 678, 1318, 1455, 644, 166, 728, 1411, 390, 996, 1139, 1318, 1213, 166, 494, 1262, 724, 877, 1318, 1213, 166, 965, 29, 1427, 545, 823, 37], [218, 207, 1055, 897, 1152, 1390, 166, 682, 1046, 945, 1400, 1035, 834, 1348, 1442, 245, 1335, 1386, 932, 1001, 293, 1167, 672, 166, 1411, 227, 1363, 1289, 509, 1374, 1242], [275, 897, 351, 166, 358, 693, 166, 682, 930, 1001, 248, 1145, 897, 13, 82, 1424, 848, 852, 275, 857, 235, 437, 475, 166, 1135, 1419, 1188, 754, 437, 970, 14, 437, 538, 857], [460, 582, 915, 139, 1424, 9, 377, 897, 1263, 166, 358, 134, 1173, 1062, 681, 320, 1333, 1224, 877, 532, 1318, 1106, 142, 1374, 131, 1065, 28, 2, 877, 1173, 1062, 358, 850, 275, 465], [275, 207, 893, 868, 897, 295, 359, 1419, 365, 1318, 1459, 166, 1199, 1435, 320, 1318, 1266, 15, 1110, 1423, 1025, 989, 437, 410, 1403, 877, 320, 1424, 993, 1419, 897, 1282, 166, 897, 84, 408, 569, 1132, 275, 400, 1374, 792, 583, 166, 1001, 492], [1228, 146, 723, 1424, 428, 375, 1318, 376, 1440, 1413, 1350, 1049, 275, 1318, 282, 1018, 1457, 1150, 1149, 807, 15, 419, 877, 494, 1386, 667, 275, 1318, 624, 876, 772, 1244, 43, 218, 1318, 413, 1182, 55], [1419, 1001, 435, 275, 822, 647, 1318, 24, 166, 1001, 714, 409, 859, 877, 820, 166, 897, 860, 1419, 1318, 686, 748, 856, 320, 218, 1200, 166, 926, 166, 207, 1238, 483, 275, 897, 1127, 166, 1118, 241, 275, 207, 67, 1440, 897, 1279, 19, 437, 1269, 166, 885], [741, 1393, 320, 358, 1140, 1422, 1153, 1001, 240, 947, 1278, 166, 897, 1097, 877, 358, 212, 166, 358, 1170, 1163, 166, 1303, 299, 166, 45], [1318, 656, 589, 897, 455, 877, 897, 969, 1374, 1044, 877, 1303, 1451, 15, 1249, 239, 103, 877, 1318, 101, 1323, 166, 880, 479, 877, 1312, 1287], [1318, 1458, 166, 637, 1042, 705, 1267, 1419, 1001, 226, 1374, 694, 689, 877, 701, 320, 1101, 499, 166, 1180, 598, 877, 499, 1002, 1373, 840, 218, 436], [1318, 370, 1424, 273, 275, 897, 767, 262, 1318, 462, 166, 682, 930, 877, 275, 610, 897, 642, 1030, 581, 1442, 508, 1050, 984, 264, 757, 1374, 766], [868, 897, 661, 166, 1199, 608, 1424, 450, 275, 1378, 412, 1065, 750, 1388, 275, 1035, 166, 1001, 1098, 130, 166, 897, 1127, 877, 320, 556, 1065, 6, 166, 634, 670, 877, 166, 636, 327, 427], [1395, 166, 171, 166, 1329, 26, 1424, 516, 275, 1318, 104, 1450, 1318, 378, 166, 897, 638, 373, 218, 1186, 877, 473, 1022, 1285, 582, 867, 139], [1318, 78, 275, 1318, 725, 166, 1088, 81, 480, 1318, 870, 1338, 1226, 1386, 795, 1419, 358, 1360, 166, 732, 320, 418, 166, 231, 52, 877, 157, 275, 207, 245], [1030, 1253, 320, 1265, 1008, 1010, 1386, 358, 56, 1424, 839, 1419, 1368, 320, 1156, 1397, 1419, 746, 207, 1221, 1419, 1347, 510, 17, 1140, 1376, 320, 711, 1068, 798, 1419, 1148, 166, 207, 210, 1089], [358, 808, 897, 1340, 166, 641, 166, 740, 707, 275, 32, 1121, 275, 897, 955, 661, 550, 685, 1050, 39, 1419, 358, 306, 1318, 888, 166, 814, 275, 1318, 1314, 42, 166, 878], [1416, 122, 897, 662, 902, 951, 1424, 332, 218, 1318, 551, 1442, 258, 877, 179, 320, 1256, 522, 1050, 207, 1308, 50, 1218, 1374, 621, 1419, 386, 1442, 320, 912, 230, 1387, 1325, 992], [1318, 1160, 209, 207, 132, 1442, 300, 1442, 928, 1035, 959, 1442, 1392, 166, 855, 166, 300, 275, 1318, 1294, 1209, 760, 1230, 275, 1318, 787], [207, 844, 203, 320, 358, 931, 230, 548, 1001, 1144, 1419, 777, 877, 722, 1424, 1096, 877, 275, 1318, 898, 437, 1137, 166, 1249, 1073, 877, 36, 1419, 1001, 1309, 1374, 597, 166, 897, 255], [882, 320, 275, 126, 543, 1424, 498, 1318, 204, 166, 595, 243, 166, 1276, 307, 398, 227, 949, 877, 222, 275, 835, 623, 467, 166, 897, 1341, 344, 166, 1191], [826, 474, 1455, 644, 166, 1009, 1112, 1419, 682, 930, 1030, 1305, 166, 942, 877, 786, 260, 897, 813, 166, 781, 437, 1318, 1213, 1205, 254, 111], [275, 1065, 162, 525, 1419, 1126, 1318, 1344, 275, 1053, 101, 1191, 506, 348, 1419, 843, 166, 1124, 1295, 1386, 897, 301, 166, 1001, 731, 1207, 166, 521, 1431, 1260, 172, 877, 397, 1056, 882, 1216, 1318, 520, 98], [1318, 1458, 166, 637, 1042, 705, 1267, 1419, 1001, 226, 1374, 694, 689, 877, 701, 320, 1101, 499, 166, 1180, 598, 877, 499, 1002, 1373, 840, 218, 436], [1386, 1318, 3, 166, 422, 897, 456, 877, 946, 1318, 694, 166, 897, 1190, 275, 1318, 11, 166, 1180, 178, 877, 275, 1315, 437, 1001, 1020, 107, 1298, 1035, 478, 1380, 275, 538, 1430], [1228, 146, 723, 1424, 428, 375, 1318, 376, 1440, 1413, 1350, 1049, 275, 1318, 282, 1018, 1457, 1150, 1149, 807, 15, 419, 877, 494, 1386, 667, 275, 1318, 624, 876, 772, 1244, 43, 218, 1318, 413, 1182, 55], [1416, 122, 897, 662, 902, 951, 1424, 332, 218, 1318, 551, 1442, 258, 877, 179, 320, 1256, 522, 1050, 207, 1308, 50, 1218, 1374, 621, 1419, 386, 1442, 320, 912, 230, 1387, 1325, 992], [897, 968, 1365, 1374, 1288, 431, 1017, 607, 437, 1318, 557, 15, 1318, 686, 466, 311, 609, 400, 1318, 287, 952, 123, 686, 275, 1318, 1238, 33, 5, 356, 166, 897, 1127, 166, 1326, 1419, 1001, 1264, 936], [897, 161, 1374, 1459, 1018, 166, 358, 1278, 276, 218, 1001, 280, 1437, 1272, 877, 1318, 557, 1370, 1302, 841, 744, 789, 9, 275, 1420, 15, 897, 176, 1129, 1419, 1148, 1374, 555, 165, 1447, 8, 877, 1065, 1268, 166, 424, 320, 50, 15, 358, 664, 1105, 791, 1155], [1395, 166, 171, 166, 1329, 26, 1424, 516, 275, 1318, 104, 1450, 1318, 378, 166, 897, 638, 373, 218, 1186, 877, 473, 1022, 1285, 582, 867, 139], [1318, 606, 1265, 1324, 218, 1318, 1182, 1072, 615, 358, 191, 227, 526, 877, 961, 702, 1318, 225, 117, 514, 1318, 155, 1382, 390, 380, 877, 1318, 431, 1382, 1123, 564, 416, 776, 1419, 897, 223, 623, 428, 1280, 1419, 358, 1049, 437, 1180, 827, 868, 504, 17], [358, 808, 897, 1340, 166, 641, 166, 740, 707, 275, 32, 1121, 275, 897, 955, 661, 550, 685, 1050, 39, 1419, 358, 306, 1318, 888, 166, 814, 275, 1318, 1314, 42, 166, 878], [1001, 293, 1054, 1229, 1428, 881, 121, 1094, 897, 286, 166, 207, 557, 166, 320, 493, 1113, 1318, 334, 166, 897, 633, 1419, 1368, 166, 320, 207, 557, 121, 1401, 1220, 437, 251, 659, 1093, 166, 897, 633, 656, 940, 505, 320, 1100, 1300, 659], [897, 1402, 1133, 1419, 320, 533, 1001, 335, 1374, 376, 531, 166, 1338, 363, 166, 842, 207, 828, 1386, 64, 497, 395, 275, 358, 1343, 1374, 1254], [1318, 1160, 218, 490, 1123, 168, 877, 1318, 1074, 218, 1240, 1192, 591, 1291, 333, 1239, 1318, 319, 166, 484, 1442, 557, 218, 897, 411, 300, 166, 1385], [826, 474, 1455, 644, 166, 1009, 1112, 1419, 682, 930, 1030, 1305, 166, 942, 877, 786, 260, 897, 813, 166, 781, 437, 1318, 1213, 1205, 254, 111], [1318, 1459, 337, 166, 688, 1424, 519, 1035, 161, 275, 897, 379, 1164, 36, 722, 275, 160, 166, 451, 877, 1366, 1091, 320, 1424, 426, 897, 562, 166, 170], [192, 1025, 1035, 303, 166, 561, 320, 534, 705, 352, 97, 218, 358, 94, 320, 414, 897, 829, 166, 1180, 1057, 705, 489, 1318, 152, 199, 1382, 361], [1318, 847, 1215, 139, 166, 320, 1318, 1213, 929, 384, 471, 1031, 1065, 101, 694, 166, 652, 275, 12, 1419, 897, 27, 166, 897, 1111, 166, 781, 218, 1055, 166, 309], [1001, 1020, 1069, 1318, 838, 166, 453, 877, 879, 1419, 897, 379, 875, 780, 202, 320, 897, 813, 1371, 321, 711, 449, 371, 897, 456, 166, 801, 1419, 897, 1352, 166, 358, 579, 1059, 166, 436, 408, 1424, 1134, 112, 1318, 1149, 166, 897, 1307, 938, 218, 1318, 918], [460, 582, 915, 139, 1424, 9, 377, 897, 1263, 166, 358, 134, 1173, 1062, 681, 320, 1333, 1224, 877, 532, 1318, 1106, 142, 1374, 131, 1065, 28, 2, 877, 1173, 1062, 358, 850, 275, 465], [897, 334, 166, 494, 1147, 93, 546, 554, 275, 897, 654, 166, 897, 1390, 166, 682, 1046, 1152, 1001, 1342, 886, 1000, 1374, 1289, 509, 166, 1198, 1242, 390, 1271, 877, 1035, 350, 765, 166, 697, 166, 897, 796, 275, 400, 1374, 694, 225], [1001, 857, 320, 1201, 620, 1054, 513, 1419, 1065, 1243, 14, 1164, 805, 1014, 75, 1318, 1358, 1168, 1419, 297, 448, 857, 1067, 1376, 805, 1424, 1367, 1318, 995, 166, 1035, 783, 256, 882, 389, 1442, 1243], [897, 864, 558, 1374, 1187, 34, 1391, 985, 616, 976, 100, 275, 405, 128, 1233, 958, 877, 861, 460, 49, 1002, 1050, 1080, 794, 1247, 166, 265, 877, 91], [207, 844, 203, 320, 358, 931, 230, 548, 1001, 1144, 1419, 777, 877, 722, 1424, 1096, 877, 275, 1318, 898, 437, 1137, 166, 1249, 1073, 877, 36, 1419, 1001, 1309, 1374, 597, 166, 897, 255], [506, 1025, 1055, 166, 1065, 1268, 166, 603, 320, 83, 437, 470, 1419, 999, 166, 1029, 458, 79, 166, 1035, 1184, 166, 1174, 877, 909, 320, 1261, 1419, 410, 166, 358, 867, 1332], [1318, 1160, 209, 207, 132, 1442, 300, 1442, 928, 1035, 959, 1442, 1392, 166, 855, 166, 300, 275, 1318, 1294, 1209, 760, 1230, 275, 1318, 787], [1318, 374, 166, 1327, 166, 409, 1318, 1323, 166, 819, 1102, 877, 189, 1337, 1367, 897, 1453, 166, 1065, 680, 567, 275, 1065, 1273, 690, 322, 275, 897, 719, 1128, 320, 1318, 1455, 862], [1001, 857, 320, 1201, 620, 1054, 513, 1419, 1065, 1243, 14, 1164, 805, 1014, 75, 1318, 1358, 1168, 1419, 297, 448, 857, 1067, 1376, 805, 1424, 1367, 1318, 995, 166, 1035, 783, 256, 882, 389, 1442, 1243], [358, 268, 982, 1424, 1146, 853, 218, 897, 1304, 1434, 320, 1318, 1358, 890, 1050, 253, 764, 684, 320, 47, 1159, 166, 897, 339, 553, 166, 1219, 877, 320, 193, 166, 1249, 1054, 756], [897, 392, 166, 642, 54, 986, 289, 873, 1265, 124, 1440, 1318, 1323, 798, 587, 218, 358, 755, 166, 700, 922, 175, 166, 1410, 1277, 845, 166, 244, 877, 194, 275, 432, 437, 207, 644, 166, 1452, 1209, 523, 877, 1306, 74, 1374, 694], [1171, 1424, 113, 1035, 935, 105, 1419, 1034, 96, 1022, 1025, 1318, 1392, 166, 897, 224, 275, 1318, 205, 551, 166, 1446, 877, 650, 1026, 1442, 1179, 1381, 218, 207, 1194, 1169, 275, 1318, 541, 1386, 320, 618, 1203, 1318, 1455, 599, 166, 897, 1085], [897, 1402, 1133, 1419, 320, 533, 1001, 335, 1374, 376, 531, 166, 1338, 363, 166, 842, 207, 828, 1386, 64, 497, 395, 275, 358, 1343, 1374, 1254], [275, 207, 893, 868, 897, 295, 359, 1419, 365, 1318, 1459, 166, 1199, 1435, 320, 1318, 1266, 15, 1110, 1423, 1025, 989, 437, 410, 1403, 877, 320, 1424, 993, 1419, 897, 1282, 166, 897, 84, 408, 569, 1132, 275, 400, 1374, 792, 583, 166, 1001, 492], [664, 830, 1442, 694, 674, 218, 897, 1081, 1442, 571, 166, 897, 817, 733, 166, 1198, 245, 1257, 877, 683, 1065, 184, 137, 1386, 897, 278, 166, 1198, 245, 1389, 1419, 320, 1318, 259, 1333, 422, 1318, 204, 166, 1001, 727, 316, 982], [897, 710, 679, 1424, 1232, 437, 1035, 205, 710, 262, 275, 126, 437, 358, 1360, 166, 1165, 1162, 166, 1021, 18, 166, 728, 623, 1346, 1374, 953], [166, 973, 990, 1072, 454, 1286, 166, 897, 206, 218, 1055, 166, 402, 179, 320, 64, 1060, 1419, 975, 897, 562, 1145, 320, 1318, 86, 166, 897, 385, 711, 699], [218, 711, 320, 1171, 166, 692, 877, 784, 1001, 799, 180, 939, 1318, 204, 166, 1127, 800, 166, 41, 983, 166, 1421, 509, 491, 1071, 116, 919, 166, 1237, 1109, 639, 166, 612, 877, 369, 572, 166, 897, 796, 766, 166, 835], [192, 1025, 1035, 303, 166, 561, 320, 534, 705, 352, 97, 218, 358, 94, 320, 414, 897, 829, 166, 1180, 1057, 705, 489, 1318, 152, 199, 1382, 361], [1001, 1443, 752, 805, 981, 963, 1065, 46, 267, 991, 906, 1419, 1001, 417, 877, 329, 875, 358, 1140, 1342, 1424, 981, 1141, 275, 897, 312, 877, 897, 535, 1016, 457], [166, 1315, 1442, 712, 509, 166, 897, 1190, 145, 463, 858, 1318, 346, 166, 504, 857, 1211, 892, 1001, 292, 89, 653, 1419, 1001, 565, 51, 1318, 870, 1305, 166, 942], [960, 1065, 185, 166, 1041, 275, 1318, 449, 70, 1318, 739, 1370, 1382, 1321, 877, 1318, 1250, 342, 823, 642, 1022, 837, 1318, 1037, 166, 1321, 915, 363, 166, 1004, 1241], [897, 547, 705, 340, 166, 320, 1318, 163, 665, 275, 1318, 245, 705, 812, 1065, 1028, 166, 1048, 772, 849, 868, 358, 325, 1303, 299, 166, 972], [897, 811, 509, 166, 897, 1190, 38, 1353, 1030, 640, 1318, 695, 166, 504, 1312, 292, 437, 737, 648, 275, 1001, 242, 166, 692, 166, 897, 1247, 835], [788, 897, 671, 166, 358, 438, 967, 275, 1001, 1175, 166, 1122, 1318, 870, 1052, 166, 804, 1245, 336, 566, 166, 897, 633, 656, 940, 505, 985, 62, 897, 334, 166, 743, 1087, 218, 1035, 205, 633, 437, 1318, 557, 745, 140, 218, 1318, 334, 502, 625, 881], [772, 328, 877, 275, 1035, 32, 1140, 770, 1318, 376, 1424, 1353, 320, 897, 619, 1051, 1374, 118, 166, 916, 166, 358, 1372, 166, 897, 769, 542, 1156, 1265, 537, 218, 441, 275, 285, 166, 181, 218, 1318, 1221, 320, 1006, 897, 662, 619, 1419, 897, 1409], [1176, 805, 179, 320, 1009, 945, 884, 1035, 478, 660, 275, 897, 1111, 166, 781, 1206, 320, 485, 1459, 320, 957, 331, 832, 1140, 221, 1419, 1035, 478, 275, 781], [1318, 606, 1265, 1324, 218, 1318, 1182, 1072, 615, 358, 191, 227, 526, 877, 961, 702, 1318, 225, 117, 514, 1318, 155, 1382, 390, 380, 877, 1318, 431, 1382, 1123, 564, 416, 776, 1419, 897, 223, 623, 428, 1280, 1419, 358, 1049, 437, 1180, 827, 868, 504, 17], [275, 897, 486, 1374, 376, 1119, 1318, 1183, 136, 1061, 93, 85, 1424, 720, 954, 1419, 897, 1015, 275, 1318, 1172, 1374, 571, 275, 320, 1225, 275, 897, 312, 577], [1001, 1443, 752, 805, 981, 963, 1065, 46, 267, 991, 906, 1419, 1001, 417, 877, 329, 875, 358, 1140, 1342, 1424, 981, 1141, 275, 897, 312, 877, 897, 535, 1016, 457], [366, 925, 1170, 1163, 166, 565, 867, 166, 717, 1374, 508, 527, 1415, 275, 1030, 962, 383, 64, 1153, 883, 1078, 546, 554, 1318, 313, 166, 1001, 187, 476, 1386, 897, 536, 298], [1318, 507, 166, 456, 166, 897, 547, 1390, 166, 1001, 187, 476, 1379, 1318, 228, 414, 897, 749, 166, 965, 275, 433, 877, 230, 150, 1442, 694, 320, 1424, 540], [897, 968, 1365, 1374, 1288, 431, 1017, 607, 437, 1318, 557, 15, 1318, 686, 466, 311, 609, 400, 1318, 287, 952, 123, 686, 275, 1318, 1238, 33, 5, 356, 166, 897, 1127, 166, 1326, 1419, 1001, 1264, 936], [1318, 1083, 1386, 1001, 626, 1038, 877, 927, 1411, 372, 921, 275, 999, 1374, 1248, 320, 1265, 487, 1386, 1318, 1408, 166, 934, 391, 59, 320, 358, 768, 48, 347, 1386, 1320, 64, 529, 1442, 214, 798], [402, 158, 1419, 1384, 1442, 573, 303, 166, 897, 709, 1181, 1374, 76, 877, 1050, 610, 562, 1318, 238, 1371, 1145, 1001, 197, 17, 139, 166, 897, 1066], [675, 234, 586, 1374, 997, 317, 166, 576, 203, 320, 1419, 590, 166, 1398, 1424, 382, 1035, 1117, 1140, 1095, 414, 897, 813, 440, 166, 717, 1374, 508, 218, 1339, 897, 339, 272, 708, 1419, 381, 211, 1107, 1281], [1001, 1020, 1069, 1318, 838, 166, 453, 877, 879, 1419, 897, 379, 875, 780, 202, 320, 897, 813, 1371, 321, 711, 449, 371, 897, 456, 166, 801, 1419, 897, 1352, 166, 358, 579, 1059, 166, 436, 408, 1424, 1134, 112, 1318, 1149, 166, 897, 1307, 938, 218, 1318, 918], [897, 834, 166, 1222, 166, 897, 547, 1386, 236, 518, 221, 166, 1404, 338, 166, 71, 135, 1318, 1375, 1374, 962, 1145, 1318, 327, 166, 1338, 1258, 631, 1318, 736, 1164, 1424, 1099, 1318, 1290, 166, 1001, 1048], [897, 161, 1374, 1459, 1018, 166, 358, 1278, 276, 218, 1001, 280, 1437, 1272, 877, 1318, 557, 1370, 1302, 841, 744, 789, 9, 275, 1420, 15, 897, 176, 1129, 1419, 1148, 1374, 555, 165, 1447, 8, 877, 1065, 1268, 166, 424, 320, 50, 15, 358, 664, 1105, 791, 1155], [1318, 694, 1003, 166, 643, 726, 1035, 751, 166, 468, 601, 655, 1213, 1355, 877, 551, 831, 166, 897, 876, 320, 1424, 763, 1369, 1318, 245, 275, 44, 1197, 1324, 1374, 1213, 166, 642, 54, 368, 979, 877, 1318, 290, 166, 691, 1382, 1072, 125], [358, 90, 452, 166, 682, 930, 779, 506, 877, 1120, 843, 281, 358, 53, 590, 1386, 207, 1220, 730, 320, 382, 599, 1030, 614, 275, 349, 153, 1450, 1292, 1180, 58, 304, 15, 421], [1318, 866, 166, 484, 166, 578, 1318, 694, 689, 203, 897, 326, 275, 1001, 782, 166, 1188, 439, 974, 877, 297, 275, 1065, 560, 166, 1236, 897, 630, 1374, 252, 762], [1318, 586, 1374, 644, 1374, 239, 1086, 305, 99, 1442, 1084, 1001, 1048, 320, 1318, 1358, 874, 1318, 673, 1043, 166, 1311, 400, 1065, 611, 1164, 148, 1278, 71, 1374, 694, 275, 1318, 95, 166, 897, 1127], [897, 334, 166, 494, 1147, 93, 546, 554, 275, 897, 654, 166, 897, 1390, 166, 682, 1046, 1152, 1001, 1342, 886, 1000, 1374, 1289, 509, 166, 1198, 1242, 390, 1271, 877, 1035, 350, 765, 166, 697, 166, 897, 796, 275, 400, 1374, 694, 225], [1030, 1448, 376, 358, 279, 981, 341, 966, 1386, 1407, 1419, 1305, 166, 141, 1419, 1180, 1312, 1020, 401, 275, 1035, 1122, 320, 985, 1318, 1455, 1149, 1386, 1357, 383, 549, 195, 1371, 1136], [897, 864, 558, 1374, 1187, 34, 1391, 985, 616, 976, 100, 275, 405, 128, 1233, 958, 877, 861, 460, 49, 1002, 1050, 1080, 794, 1247, 166, 265, 877, 91], [664, 830, 1442, 694, 674, 218, 897, 1081, 1442, 571, 166, 897, 817, 733, 166, 1198, 245, 1257, 877, 683, 1065, 184, 137, 1386, 897, 278, 166, 1198, 245, 1389, 1419, 320, 1318, 259, 1333, 422, 1318, 204, 166, 1001, 727, 316, 982], [960, 1065, 185, 166, 1041, 275, 1318, 449, 70, 1318, 739, 1370, 1382, 1321, 877, 1318, 1250, 342, 823, 642, 1022, 837, 1318, 1037, 166, 1321, 915, 363, 166, 1004, 1241], [166, 1315, 1442, 712, 509, 166, 897, 1190, 145, 463, 858, 1318, 346, 166, 504, 857, 1211, 892, 1001, 292, 89, 653, 1419, 1001, 565, 51, 1318, 870, 1305, 166, 942], [1171, 92, 782, 1374, 104, 600, 7, 1050, 187, 476, 897, 1390, 166, 358, 682, 1046, 897, 666, 836, 877, 66, 1018, 877, 110, 797, 1424, 851, 1442, 758], [166, 1055, 1374, 694, 897, 617, 166, 1408, 563, 696, 1047, 188, 1374, 557, 1161, 166, 76, 1270, 635, 275, 1001, 320, 738, 320, 1424, 16, 247, 877, 421, 218, 1318, 944, 711, 320, 1076, 1424, 121, 629, 138, 877, 1231], [904, 1371, 1318, 978, 245, 1374, 104, 1164, 1424, 908, 815, 208, 166, 923, 632, 166, 761, 1413, 1119, 211, 1442, 1459, 437, 824, 211, 394, 1145, 1318, 1119, 166, 872], [166, 973, 990, 1072, 454, 1286, 166, 897, 206, 218, 1055, 166, 402, 179, 320, 64, 1060, 1419, 975, 897, 562, 1145, 320, 1318, 86, 166, 897, 385, 711, 699], [897, 177, 447, 275, 1065, 423, 166, 442, 446, 1019, 166, 1296, 1442, 580, 166, 897, 312, 907, 734, 323, 64, 315, 877, 4, 987, 1386, 742], [897, 710, 679, 1424, 1232, 437, 1035, 205, 710, 262, 275, 126, 437, 358, 1360, 166, 1165, 1162, 166, 1021, 18, 166, 728, 623, 1346, 1374, 953], [166, 1030, 65, 1318, 186, 166, 143, 903, 948, 1333, 994, 1318, 937, 950, 275, 646, 1450, 263, 151, 275, 897, 1092, 496, 877, 530, 218, 1065, 73, 1419, 358, 472, 166, 687, 275, 1079], [1065, 61, 166, 129, 721, 60, 205, 1055, 166, 488, 897, 312, 166, 682, 930, 753, 1419, 897, 574, 166, 358, 1394, 275, 1256, 1414, 1419, 237, 358, 325, 159, 166, 677], [275, 1065, 162, 525, 1419, 1126, 1318, 1344, 275, 1053, 101, 1191, 506, 348, 1419, 843, 166, 1124, 1295, 1386, 897, 301, 166, 1001, 731, 1207, 166, 521, 1431, 1260, 172, 877, 397, 1056, 882, 1216, 1318, 520, 98], [895, 437, 897, 367, 166, 387, 1054, 622, 275, 1399, 69, 166, 897, 312, 1171, 166, 320, 275, 897, 1274, 563, 1439, 868, 897, 942, 1433, 1424, 154, 1322, 1318, 1317, 1064, 387, 897, 1058, 166, 1377, 1386, 198], [554, 1265, 1070, 275, 1275, 1374, 1294, 166, 897, 1190, 1318, 291, 166, 84, 320, 810, 1396, 1318, 21, 174, 754, 1012, 15, 857, 1374, 896, 1266], [897, 828, 1374, 694, 481, 139, 166, 897, 1449, 1374, 644, 166, 785, 459, 802, 1193, 1419, 488, 1386, 771, 437, 897, 1023, 166, 344, 68, 539, 1177], [897, 811, 509, 166, 897, 1190, 38, 1353, 1030, 640, 1318, 695, 166, 504, 1312, 292, 437, 737, 648, 275, 1001, 242, 166, 692, 166, 897, 1247, 835]]\n","[[1140, 1376, 1198, 1459, 1424, 1367, 1065, 680, 166, 1, 1, 275, 1318, 1, 166, 488, 1, 218, 711, 320, 1371, 1318, 1455, 680, 275, 1], [358, 1, 1374, 1, 166, 1, 1, 544, 1065, 1, 754, 1, 275, 897, 1, 1065, 1253, 166, 1, 754, 1, 166, 897, 1, 877, 1, 1374, 1], [1318, 1, 166, 1097, 877, 1, 563, 1374, 1185, 821, 1, 877, 897, 1097, 1, 166, 1, 1, 1, 1, 414, 778, 877, 204, 166, 1, 1, 1, 877, 1, 63, 877, 1, 1, 1, 1, 1386, 1], [64, 945, 1, 383, 1318, 1, 121, 344, 275, 1, 1, 1202, 383, 1188, 1371, 1318, 599, 166, 1, 166, 897, 1, 754, 82, 1318, 599, 1164, 1424, 1, 218, 662, 1062], [275, 688, 558, 1, 1419, 1, 1, 1065, 1, 166, 859, 1, 275, 1, 437, 1318, 253, 166, 1, 383, 1371, 1, 275, 1318, 204, 166, 897, 1], [897, 903, 1, 166, 1, 1, 1419, 897, 828, 166, 897, 1, 275, 484, 166, 1, 1318, 252, 1050, 1035, 652, 166, 1097, 1, 166, 1, 1018], [358, 1, 437, 1233, 218, 1, 1, 1, 1, 1065, 205, 1, 877, 1, 166, 1, 1386, 1, 1, 877, 1, 320, 1424, 1, 1442, 239, 1374, 1037], [1318, 327, 166, 484, 166, 578, 1318, 1, 166, 1, 1374, 1, 1, 1018, 1, 166, 897, 1390, 317, 166, 897, 1097, 1, 203, 1035, 652, 166, 1097, 1018, 218, 1318, 252, 166, 1], [1035, 219, 762, 1, 275, 1, 697, 437, 1, 414, 897, 1, 166, 1, 1, 1, 1, 754, 1, 166, 1, 1050, 1, 1386, 1, 1065, 1, 754, 1236, 358, 1, 1374, 63], [437, 13, 869, 166, 484, 166, 578, 1424, 848, 1, 1, 275, 1, 1, 1, 1, 1, 904, 1374, 508, 436, 558, 1, 558, 1, 558, 1, 1, 558, 1, 558, 682, 930, 1185, 821, 1, 1446, 1, 877, 743], [1, 1, 39, 1, 877, 1, 166, 1, 1, 1, 1, 1102, 1, 1, 15, 1306, 981, 341, 1, 754, 1, 218, 1, 166, 897, 1], [33, 1, 586, 166, 897, 1390, 317, 166, 897, 1097, 1, 1393, 320, 1371, 1, 1, 1, 275, 599, 166, 1, 1386, 1, 358, 1, 1374, 63], [1386, 1318, 531, 166, 688, 166, 578, 358, 1, 1, 166, 1, 1318, 104, 1, 358, 1, 1, 166, 1035, 942, 135, 897, 1027, 1, 166, 1], [1167, 1, 1, 275, 1318, 245, 1, 59, 981, 1, 166, 1001, 1, 1, 857, 320, 1424, 981, 1, 1424, 1, 448, 1, 338, 1, 1, 166, 1, 275, 897, 1, 166, 1], [1035, 942, 139, 1065, 1, 1, 1, 166, 1, 1419, 1, 135, 897, 1127, 166, 1188, 558, 1265, 322, 877, 1424, 1367, 320, 1025, 1318, 1, 680, 166, 1, 1, 275, 1], [1318, 1, 166, 872, 358, 74, 166, 358, 1, 1, 320, 1318, 1, 1424, 1132, 1, 275, 897, 1, 1371, 1, 320, 1424, 1, 166, 256, 1419, 256, 437, 1035, 1, 1, 877, 320, 1, 1, 1, 275, 358, 1, 1], [875, 743, 1424, 716, 15, 358, 1, 697, 437, 1318, 1, 1028, 166, 1, 1, 275, 1318, 104, 1145, 1318, 1, 166, 872, 82, 1424, 1, 582, 338, 437, 1035, 1, 166, 1, 1374, 1], [1318, 1, 705, 1, 1, 275, 897, 1127, 166, 1188, 877, 1, 981, 1094, 207, 1, 414, 897, 1, 166, 897, 1127, 1386, 1, 1318, 252], [297, 1, 1419, 1, 1, 166, 897, 1, 1, 420, 437, 897, 1, 420, 275, 1318, 1, 1, 1, 1, 877, 420, 275, 358, 1, 877, 1001, 1], [1318, 1, 166, 1, 166, 1, 1001, 1020, 1, 166, 897, 1127, 166, 1188, 1, 414, 897, 829, 166, 1, 857, 1, 166, 1, 1, 1, 46, 166, 1, 1], [897, 256, 1065, 1, 166, 1413, 1, 867, 1265, 1, 166, 1, 1374, 1243, 1318, 1, 166, 688, 275, 1, 1, 1386, 1, 275, 1318, 1], [897, 1390, 317, 166, 897, 1097, 1, 1270, 1065, 758, 1419, 358, 495, 166, 697, 166, 1, 1419, 1, 1440, 1318, 1, 166, 1, 166, 1, 166, 1, 166, 63], [1, 1424, 1367, 320, 1318, 1220, 680, 1025, 1065, 1, 1374, 1354, 1, 166, 358, 1, 275, 207, 1283, 166, 1, 166, 1274, 275, 1, 1, 488], [275, 688, 166, 409, 1318, 1455, 680, 275, 1318, 1185, 821, 1374, 101, 627, 1265, 1, 275, 1, 275, 1065, 1, 166, 45, 1, 320, 534, 121, 1, 1318, 1, 1, 877, 1, 1265, 1318, 1, 680, 1, 1419, 446, 1], [1, 1, 1455, 644, 1, 705, 1, 1035, 860, 166, 1, 320, 64, 1, 1, 877, 320, 1153, 1, 1050, 598, 1, 166, 1, 1, 166, 1, 1, 1050, 897, 1, 166, 1], [897, 129, 166, 897, 1, 166, 627, 705, 1, 1419, 1178, 1, 735, 1419, 1, 1, 1180, 1, 166, 120, 275, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 877, 1, 1, 1], [358, 764, 325, 1, 166, 897, 1, 166, 1, 218, 627, 166, 1, 275, 1, 1424, 1, 1318, 1, 166, 872, 166, 578, 1258, 1030, 1318, 1, 245, 166, 479, 1, 1442, 449, 1, 1318, 1], [1262, 1, 1, 166, 778, 877, 1, 166, 897, 1097, 275, 897, 1, 166, 1097, 166, 1191, 701, 320, 1442, 1, 320, 1306, 697, 1001, 1, 1, 1, 320, 897, 1, 166, 63, 271, 1, 218, 1, 1], [1167, 1318, 1, 1, 1, 655, 79, 166, 358, 1, 1386, 1318, 204, 877, 897, 778, 166, 1154, 166, 358, 682, 930, 897, 1, 64, 1134, 849], [1442, 866, 166, 484, 166, 578, 1318, 101, 627, 1424, 121, 1, 1419, 1178, 1, 877, 782, 735, 275, 558, 437, 1, 1, 1, 877, 1, 338, 1]]\n","1\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00,  8.31it/s]\n"]}]},{"cell_type":"code","source":["# HASTA AQUÍ"],"metadata":{"id":"cCPT8jX81U1D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df_cord_dev[\"nombre_fichero\"] = \"cord\"\n","df_wikinews_dev[\"nombre_fichero\"] = \"wikinews\"\n","df_medline_dev[\"nombre_fichero\"] = \"medline\"\n","N_ITERACIONES = 5\n","df_dev[[\"Sentence #\", \"fichero\", \"nombre_fichero\"]].to_csv(raiz+\"dataset_dev.csv\")\n","# \n","for iteracion in [\"MODELO_FINAL\"]:\n","  for reg in num_reg:  \n","    lim_i_vw = floor(reg/3/2)\n","    lim_f_vw = ceil(reg/3/2)\n","    lim_i = floor(reg/3)\n","    lim_f = ceil(reg/3)\n","    print(reg,lim_i_vw,  lim_f_vw, lim_i, lim_f)\n","\n","    ### dif_wv_33\n","    df_cord_wv_1 = df_cord.sort_values(\"dif_wv_33\", ascending=False).head(lim_i_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv_2 = df_cord.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv = pd.concat([df_cord_wv_1, df_cord_wv_2], axis=0).reset_index(drop=True)\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv_1 = df_medline.sort_values(\"dif_wv_33\", ascending=False).head(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv_2 = df_medline.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv = pd.concat([df_medline_wv_1, df_medline_wv_2], axis=0).reset_index(drop=True)\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv_1 = df_wikinews.sort_values(\"dif_wv_33\", ascending=False).head(lim_i_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv_2 = df_wikinews.sort_values(\"dif_wv_33\", ascending=True).tail(lim_f_vw)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv = pd.concat([df_wikinews_wv_1, df_wikinews_wv_2], axis=0).reset_index(drop=True)\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_wv = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_wv.to_csv(raiz+str(reg)+\"_dif_wv_33.csv\")\n","\n","    ### sum_freq_palabra\n","    df_cord_wv = df_cord.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_f)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sort_values(\"sum_freq_palabra\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_freq_palabra = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_freq_palabra.to_csv(raiz+str(reg)+\"_sum_freq_palabra.csv\")\n","\n","    ### sum_freq_pos\n","    df_cord_wv = df_cord.sort_values(\"sum_freq_pos\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sort_values(\"sum_freq_pos\", ascending=False).head(lim_f)[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sort_values(\"sum_freq_pos\", ascending=False).head(lim_i)[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_freq_pos = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_freq_pos.to_csv(raiz+str(reg)+\"_sum_freq_pos.csv\")\n","\n","    ### random\n","    df_cord_wv = df_cord.sample(min(lim_i, len(df_cord) ))[[\"Sentence #\", \"fichero\"]]\n","    df_cord_wv[\"nombre_fichero\"] = \"cord\"\n","\n","    df_medline_wv = df_medline.sample(min(lim_i, len(df_medline_wv) ))[[\"Sentence #\", \"fichero\"]]\n","    df_medline_wv[\"nombre_fichero\"] = \"medline\"\n","\n","    df_wikinews_wv = df_wikinews.sample(min(lim_i, len(df_wikinews_wv) ))[[\"Sentence #\", \"fichero\"]]\n","    df_wikinews_wv[\"nombre_fichero\"] = \"wikinews\"\n","    dataset_random = pd.concat([df_cord_wv, df_medline_wv, df_wikinews_wv], axis=0).drop_duplicates().reset_index(drop=True)\n","    dataset_random.to_csv(raiz+str(reg)+\"_random.csv\")\n","\n","    print(len(dataset_wv), len(dataset_freq_palabra), len(dataset_freq_pos), len(dataset_random))\n","\n","    sufijo = \"_preprocessing.csv\"\n","    archivos = [\"wikinews.25.txt\",\"wikinews.75.txt\",\"wikinews.300.es.txt\",\n","                \"medline.75.txt\",\"medline.1200.es.txt\",\n","                \"cord.150.txt\"]\n","    for i, archivo in enumerate(archivos):\n","      if i == 0:\n","        df = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","        df[\"fichero\"] = archivo.split(\".\")[1]\n","        df[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","      else:\n","        df_tmp = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","        df_tmp[\"fichero\"] = archivo.split(\".\")[1]\n","        df_tmp[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","        df = pd.concat([df,df_tmp], axis=0).reset_index(drop=True)\n","    # lim_i, lim_f = (-1, 25)\n","    lim_i, lim_f = (25, 50)\n","\n","    df_dev_seleccion = pd.read_csv(raiz+\"dataset_dev.csv\", index_col=0)\n","    df[\"fichero\"] = df[\"fichero\"].astype(int)\n","    df_dev = pd.merge(df, df_dev_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","    print(len(df_dev))\n","    df_dev = df_dev.loc[(df_dev[\"longitud\"] > lim_i) & (df_dev[\"longitud\"] <= lim_f), :].copy()\n","    df_dev[\"Sentence #\"] = df_dev[\"Sentence #\"].astype(str) + \"_\" + df_dev[\"fichero\"].astype(str)  + \"_\" + df_dev[\"nombre_fichero\"]\n","    print(len(df_dev))\n","    df_ingles_dev = df_dev.loc[df_dev[\"nombre_fichero\"] == \"cord\"]\n","    df_espanol_dev = df_dev.loc[df_dev[\"nombre_fichero\"] != \"cord\"]\n","    # Seleccionar las oraciones que forman cada dataframe. '_random.csv'\n","    sufijos = ['_sum_freq_pos.csv']\n","    BS = 32\n","    NUM_EPOCHS = 450\n","    for suf in sufijos:\n","      num = reg\n","      resultado = \"resultado_\"+str(num)+suf\n","      df_seleccion = pd.read_csv(raiz+str(num)+suf, index_col=0)\n","      df[\"fichero\"] = df[\"fichero\"].astype(int)\n","      df_train = pd.merge(df, df_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","      print(len(df_train))\n","      df_train = df_train.loc[(df_train[\"longitud\"] > lim_i) & (df_train[\"longitud\"] <= lim_f), :].copy()\n","      df_train[\"Sentence #\"] = df_train[\"Sentence #\"].astype(str) + \"_\" + df_train[\"fichero\"].astype(str) + \"_\" + df_train[\"nombre_fichero\"]\n","      print(len(df_train))\n","      df_ingles_analiza = df_train.loc[df_train[\"nombre_fichero\"] == \"cord\"]\n","      df_espanol_analiza = df_train.loc[df_train[\"nombre_fichero\"] != \"cord\"]\n","\n","      # Entrenar con los distintos dataframes\n","      modelo_en = str(num) + \"_bert_en\" + str(lim_f) + suf.split(\".\")[0] + \"_iter\"+str(iteracion)\n","      modelo_es = str(num) + \"_bert_es\" + str(lim_f)  + suf.split(\".\")[0] + \"_iter\"+str(iteracion)\n","      raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","      log_file = \"/content/drive/MyDrive/COLAB - TFM/full_training\"+ suf \n","      df_cruce_ingles = df_ingles_analiza\n","      df_cruce_espanol = df_espanol_analiza\n","      for data, modelo, MAX_LEN, lan, data_dev in ([df_cruce_espanol.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]], raiz + modelo_es, lim_f, \"es\", df_espanol_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]],                                \n","                                    [df_cruce_ingles.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],raiz+modelo_en, lim_f, \"en\", df_ingles_dev.loc[:,[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]]):\n","        with open(log_file, \"a+\") as f:\n","          f.write(\";\".join([str(datetime.datetime.now()), suf, modelo, \"\\n\"]))\n","        if len(data) > 0:\n","          print(modelo)\n","          print(len(data))\n","          tag2idx = {'B-Reference': 0, 'I-Concept': 1, 'O': 2, 'I-Predicate': 3, 'B-Action': 4, 'B-Predicate': 5, 'B-Concept': 6, 'I-Action': 7}\n","          tags_vals = ['B-Reference', 'I-Concept', 'O', 'I-Predicate', 'B-Action', 'B-Predicate', 'B-Concept', 'I-Action']\n","          train_data, train_sampler, train_dataloader, tag2idx_tr, device, tags_vals_tr, idx2word_tr, tr_inputs, word2idx = bert_formating_one_dataset(data, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS)\n","          print(tag2idx, tags_vals)\n","          valid_data, valid_sampler, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, word2idx = bert_formating_one_dataset(data_dev, MAX_LEN, tag2idx = tag2idx, tags_vals=tags_vals, bs = BS,word2idx=word2idx)\n","          print(tag2idx, tags_vals)\n","          if os.path.exists(modelo):\n","            with open(log_file, \"a+\") as f:\n","              f.write(\"Escojo modelo existente: \" +  modelo +\"\\n\")\n","            model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = modelo)\n","          else:\n","            model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True)\n","          training(model, optimizer,train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = NUM_EPOCHS,\n","                      max_grad_norm = 1.0, save_path = modelo, early_stopping=NUM_EPOCHS)\n","          pred_tags, valid_tags, valid_inputs = evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n","                      guarda_resultado=raiz+resultado+\"_validacion_\"+str(MAX_LEN) + \"_iter\"+str(iteracion)+\"_\"+lan+\".csv\")\n","          pred_tags, valid_tags, valid_inputs = evaluate(model, train_dataloader, tag2idx_tr, device, tags_vals_tr, idx2word_tr, tr_inputs, save_path = modelo,\n","                      guarda_resultado=raiz+resultado+\"_entrenamiento_\"+str(MAX_LEN) + \"_iter\"+str(iteracion)+\"_\"+lan+\".csv\")\n","          os.system(\"zip \"+modelo)\n","        else:\n","          print(\"No hay datos\")\n","      clear_output()"],"metadata":{"id":"klEi1ZMWuC3x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"zvzse0wMIefT"}}]}