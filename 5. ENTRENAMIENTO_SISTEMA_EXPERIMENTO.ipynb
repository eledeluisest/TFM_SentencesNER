{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWha9kk/0Lf34m+l/M31oX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVVo-N22oBEI","executionInfo":{"status":"ok","timestamp":1662068273908,"user_tz":-120,"elapsed":22067,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"578a8024-5566-431c-935b-9d9377f6838f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 8.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Collecting boto3\n","  Downloading boto3-1.24.65-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 53.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n","Collecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.0 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting botocore<1.28.0,>=1.27.65\n","  Downloading botocore-1.27.65-py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 59.2 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.65->boto3->pytorch-pretrained-bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 55.3 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.65->boto3->pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 66.9 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.24.65 botocore-1.27.65 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=501fe0ea74f7c372f6f3550a6d1cf67ef385ddaae51ce8f784097e15d925c0d3\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 6.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=07793b026ea1ef6459740b7abfa7d6adf66e3e7c3f2e9d14fa6d749bcbf81a2c\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["# Instalaciones\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","!pip install langdetect\n"]},{"cell_type":"code","source":["# Importaciones\n","# GENERAL Y PREPROCESADO\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","import spacy\n","!python -m spacy download es_core_news_sm\n","import es_core_news_sm\n","from langdetect import detect\n","import os.path\n","import datetime\n","# BERT\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","from seqeval.metrics import f1_score\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObE8soCcoS9M","executionInfo":{"status":"ok","timestamp":1662068298623,"user_tz":-120,"elapsed":24723,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"a2ffa8e9-26aa-4112-9573-4ad969b2c572"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-09-01 21:38:10.402291: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n"]}]},{"cell_type":"code","source":["# Conexion a drive y descompresión de los corpus\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nu02Cu8roWKV","executionInfo":{"status":"ok","timestamp":1662068319778,"user_tz":-120,"elapsed":21165,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"dd3c4ef2-f8df-4db7-b928-0c906a544a6d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/COLAB\\ -\\ TFM/corpora-master.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6itlXjaoZFp","executionInfo":{"status":"ok","timestamp":1662068322089,"user_tz":-120,"elapsed":2323,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"017632f2-7f2f-4296-d486-9f8eabbe705c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/COLAB - TFM/corpora-master.zip\n","370a87a85d1d24d418a86abbe1f6918383362467\n","   creating: corpora-master/\n","  inflating: corpora-master/.gitignore  \n","   creating: corpora-master/.vscode/\n","  inflating: corpora-master/.vscode/settings.json  \n","   creating: corpora-master/2021/\n","  inflating: corpora-master/2021/README.md  \n","   creating: corpora-master/2021/eval/\n","   creating: corpora-master/2021/eval/develop/\n","  inflating: corpora-master/2021/eval/develop/README.md  \n","   creating: corpora-master/2021/eval/develop/scenario1-main/\n"," extracting: corpora-master/2021/eval/develop/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/develop/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/testing/\n","  inflating: corpora-master/2021/eval/testing/README.md  \n","   creating: corpora-master/2021/eval/testing/scenario1-main/\n"," extracting: corpora-master/2021/eval/testing/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/testing/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/training/\n","   creating: corpora-master/2021/eval/training/scenario1-main/\n"," extracting: corpora-master/2021/eval/training/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/training/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/ref/\n","   creating: corpora-master/2021/ref/develop/\n","  inflating: corpora-master/2021/ref/develop/README.md  \n","  inflating: corpora-master/2021/ref/develop/cord.50.ann  \n","  inflating: corpora-master/2021/ref/develop/cord.50.txt  \n","  inflating: corpora-master/2021/ref/develop/medline.25.ann  \n","  inflating: corpora-master/2021/ref/develop/medline.25.txt  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.ann  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.txt  \n","   creating: corpora-master/2021/ref/testing/\n","  inflating: corpora-master/2021/ref/testing/README.md  \n","  inflating: corpora-master/2021/ref/testing/cord.150.ann  \n","  inflating: corpora-master/2021/ref/testing/cord.150.txt  \n","  inflating: corpora-master/2021/ref/testing/medline.75.ann  \n","  inflating: corpora-master/2021/ref/testing/medline.75.txt  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.ann  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.txt  \n","   creating: corpora-master/2021/ref/training/\n","  inflating: corpora-master/2021/ref/training/medline.1200.es.ann  \n","  inflating: corpora-master/2021/ref/training/medline.1200.es.txt  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.ann  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.txt  \n","   creating: corpora-master/2021/submissions/\n","   creating: corpora-master/2021/submissions/baseline/\n","   creating: corpora-master/2021/submissions/baseline/develop/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/2021/submissions/baseline/submission.zip  \n","   creating: corpora-master/2021/submissions/baseline/testing/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/LICENSE  \n","  inflating: corpora-master/README.md  \n","   creating: corpora-master/scripts/\n","  inflating: corpora-master/scripts/README.md  \n","  inflating: corpora-master/scripts/anntools.py  \n","  inflating: corpora-master/scripts/baseline.py  \n","  inflating: corpora-master/scripts/score.py  \n","  inflating: corpora-master/scripts/score_2021.py  \n"]}]},{"cell_type":"markdown","source":["## Funciones"],"metadata":{"id":"fOWqLKFeCXyt"}},{"cell_type":"code","source":["def clasifica_data(file):\n","  file_csv = file\n","  ingles = []\n","  espanol = []\n","  with open(file_csv, \"r\") as f:\n","    texto = f.read().split(\"\\n\")\n","  for frase in texto:\n","    espaciado = frase.split(\" \")\n","    idioma = detect(frase)\n","    if idioma == \"en\":\n","      ingles.append(frase)\n","    elif idioma == \"es\":\n","      espanol.append(frase)\n","  return ingles, espanol\n","\n","  \n","def data_preprocessing_es(texto, ann = None):\n","  nlp = es_core_news_sm.load()\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","def data_preprocessing_en(texto, ann = None):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","def bert_formating(data, MAX_LEN, test_size=0.2, bs = 16):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                              random_state=2022, test_size=test_size)\n","  tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                              random_state=2022, test_size=test_size)\n","  tr_inputs = torch.tensor(tr_inputs)\n","  val_inputs = torch.tensor(val_inputs)\n","  tr_tags = torch.tensor(tr_tags)\n","  val_tags = torch.tensor(val_tags)\n","  tr_masks = torch.tensor(tr_masks)\n","  val_masks = torch.tensor(val_masks)\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, valid_data, valid_sampler,\\\n","            valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs\n","\n","def bert_formating_one_dataset(data, MAX_LEN , bs = 16):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs = torch.tensor(input_ids, dtype=torch.int64)\n","  tr_tags = torch.tensor(tags, dtype=torch.int64)\n","  tr_masks = torch.tensor(attention_masks, dtype = torch.uint8)\n","\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, tag2idx, device, tags_vals, idx2word, tr_inputs\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = \"\"):\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()\n","  if reentreno:\n","    print(\"Loading existing model...\")\n","    model.load_state_dict(torch.load(save_path))\n","  if FULL_FINETUNING:\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'gamma', 'beta']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.01},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.0}\n","      ]\n","  else:\n","      param_optimizer = list(model.classifier.named_parameters()) \n","      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","  return model, optimizer\n","\n","def training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = 200,\n","             max_grad_norm = 1.0, save_path = \"./bert1\"):\n","  train_f1 = []\n","  train_losses = []\n","  val_losses = []\n","  contador_max = -1\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","      # TRAIN loop\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          # add batch to gpu\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          # forward pass\n","          loss = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask, labels=b_labels)\n","          # backward pass\n","          loss.backward()\n","          # track train loss\n","          tr_loss += loss.item()\n","          nb_tr_examples += b_input_ids.size(0)\n","          nb_tr_steps += 1\n","          # gradient clipping\n","          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","          # update parameters\n","          optimizer.step()\n","          model.zero_grad()\n","      # print train loss per epoch\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","      # VALIDATION on validation set\n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels, true_inputs = [], [],[]\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          with torch.no_grad():\n","              tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                    attention_mask=b_input_mask, labels=b_labels)\n","              logits = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          inputs = b_input_ids.to('cpu').numpy()\n","          \n","          true_inputs.append(inputs)\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.append(label_ids)\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          eval_loss += tmp_eval_loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","          \n","          nb_eval_examples += b_input_ids.size(0)\n","          nb_eval_steps += 1\n","      eval_loss = eval_loss/nb_eval_steps\n","      print(\"Validation loss: {}\".format(eval_loss))\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","      valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","      valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","      f1 = f1_score([pred_tags], [valid_tags])\n","      train_f1.append(f1)\n","      train_losses.append(tr_loss/nb_tr_steps)\n","      val_losses.append(eval_loss)\n","\n","      max_f1 = max(train_f1)\n","      if f1 == max_f1:\n","        contador_max = 1\n","        torch.save(model.state_dict(), save_path)\n","      if contador_max > 0:\n","        contador_max += 1\n","      print(\"F1-Score: \" + str(train_f1[-1]))\n","      if round(max_f1, 2) > 0.1 and contador_max > 10 :\n","        print(\"Early stopping...\")\n","        return 0\n","def evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = \"./bert1\",\n","             guarda_resultado=\"/content/drive/MyDrive/COLAB - TFM/resultado_entrenamiento1.csv\"):\n","  #evaluate model\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  model.load_state_dict(torch.load(save_path))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()  \n","  model.eval()\n","  predictions = []\n","  true_labels = []\n","  true_inputs = []\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","  print(len(valid_dataloader))\n","  for batch in tqdm(valid_dataloader):\n","      #print(len(batch))\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","\n","      with torch.no_grad():\n","          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                attention_mask=b_input_mask, labels=b_labels)\n","          logits = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","          \n","      logits = logits.detach().cpu().numpy()\n","      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      label_ids = b_labels.to('cpu').numpy()\n","      inputs = b_input_ids.to('cpu').numpy()\n","      true_inputs.append(inputs)\n","      \n","      \n","      true_labels.append(label_ids)\n","      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","\n","  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n","  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n","  valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","\n","  count = 0 \n","  all_data = 0\n","  registro_resultado = []\n","  for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n","      for k,l,kk in zip(i,j,ll):\n","        count += 1\n","        registro_resultado.append([k,l,idx2word[kk.item()]])\n","        all_data += 1\n","  df_alldata = pd.DataFrame(registro_resultado)\n","  df_alldata.columns = [\"pred_tags\", \"valid_tags\", \"val_inputs\"]\n","  df_alldata[df_alldata[\"val_inputs\"] != \"PAD\"].\\\n","        to_csv(guarda_resultado,sep=\";\", encoding = \"utf-8\")\n","  return pred_tags, valid_tags, valid_inputs\n","\n","\n"],"metadata":{"id":"wEwSrIk1oaaO","executionInfo":{"status":"ok","timestamp":1662068898029,"user_tz":-120,"elapsed":287,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["## Entrenamiento"],"metadata":{"id":"0rUf8uQhCjny"}},{"cell_type":"code","source":["raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","sufijo = \"_preprocessing.csv\"\n","archivos = [\"wikinews.25.txt\",\"wikinews.75.txt\",\"wikinews.300.es.txt\",\n","            \"medline.25.txt\",\"medline.75.txt\",\"medline.1200.es.txt\",\n","            \"cord.50.txt\",\"cord.150.txt\"]\n","for i, archivo in enumerate(archivos):\n","  if i == 0:\n","    df = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","    df[\"fichero\"] = archivo.split(\".\")[1]\n","    df[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","  else:\n","    df_tmp = pd.read_csv(raiz+archivo+sufijo, index_col=0)\n","    df_tmp[\"fichero\"] = archivo.split(\".\")[1]\n","    df_tmp[\"nombre_fichero\"] = archivo.split(\".\")[0]\n","    df = pd.concat([df,df_tmp], axis=0).reset_index(drop=True)\n","\n","df_dev_seleccion = pd.read_csv(raiz+\"dataset_dev.csv\", index_col=0)\n","df[\"fichero\"] = df[\"fichero\"].astype(int)\n","df_dev = pd.merge(df, df_dev_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","df_dev.loc[df_dev[\"longitud\"] <= 25, \"len_grupo\"] = \"Menos25\"\n","df_dev.loc[(df_dev[\"longitud\"] > 25) & (df_dev[\"longitud\"] <= 50), \"len_grupo\"] = \"entre25_50\"\n"],"metadata":{"id":"p90UG2axtG3D","executionInfo":{"status":"ok","timestamp":1662069094185,"user_tz":-120,"elapsed":229,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Seleccionar las oraciones que forman cada dataframe\n","suf = \"_sum_freq_pos.csv\"\n","num_reg = [100,  300, 700, 900, 1200, 1500]\n","for num in num_reg:\n","  resultado = \"resultado_\"+str(num)+suf\n","  df_seleccion = pd.read_csv(raiz+str(num)+suf, index_col=0)\n","  df[\"fichero\"] = df[\"fichero\"].astype(int)\n","  df_train = pd.merge(df, df_seleccion, on=[\"Sentence #\", \"fichero\", \"nombre_fichero\"], how=\"inner\")\n","  df_ingles_analiza = df_train.loc[df_train[\"nombre_fichero\"] == \"cord\"]\n","  df_espanol_analiza = df_train.loc[df_train[\"nombre_fichero\"] != \"cord\"]\n","\n"," \n","  df_ingles_dev = df_dev.loc[df_dev[\"nombre_fichero\"] == \"cord\"]\n","  df_espanol_dev = df_dev.loc[df_dev[\"nombre_fichero\"] != \"cord\"]\n","\n","  # Entrenar con los distintos dataframes\n","  modelo_en_len0_25 = str(num) + \"_bert_en_len_0_25\"\n","  modelo_en_len25_50 = str(num) + \"_bert_en_len_25_50\"\n","\n","  modelo_es_len0_25 = str(num) + \"_bert_es_len_0_25\"\n","  modelo_es_len25_50 = str(num) + \"_bert_es_len_25_50\"\n","\n","  raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","\n","  log_file = \"/content/drive/MyDrive/COLAB - TFM/full_training.log\"\n","\n","  df_tmp = df_ingles_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","  df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)  \n","  df_tmp.loc[df_tmp[\"longitud\"] <= 25, \"len_grupo\"] = \"Menos25\"\n","  df_tmp.loc[(df_tmp[\"longitud\"] > 25) & (df_tmp[\"longitud\"] <= 50), \"len_grupo\"] = \"entre25_50\"\n","  df_cruce_ingles = pd.merge(df_ingles_analiza, df_tmp[[\"Sentence #\", \"len_grupo\"]], on=[\"Sentence #\"], how=\"left\")\n","\n","  df_tmp = df_espanol_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","  df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)\n","  df_tmp.loc[df_tmp[\"longitud\"] <= 25, \"len_grupo\"] = \"Menos25\"\n","  df_tmp.loc[(df_tmp[\"longitud\"] > 25) & (df_tmp[\"longitud\"] <= 50), \"len_grupo\"] = \"entre25_50\"\n","  df_cruce_espanol = pd.merge(df_espanol_analiza, df_tmp[[\"Sentence #\", \"len_grupo\"]], on=[\"Sentence #\"], how=\"left\")\n","\n","\n","  NUM_EPOCHS = 200  \n","  for data, modelo, MAX_LEN, lan, data_dev in ([df_cruce_espanol.loc[df_cruce_espanol[\"len_grupo\"] == \"Menos25\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],\n","                                            raiz+modelo_es_len0_25, 25, \"es\", df_espanol_dev.loc[df_espanol_dev[\"len_grupo\"] == \"Menos25\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]],\n","                                [df_cruce_espanol.loc[df_cruce_espanol[\"len_grupo\"] == \"entre25_50\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],\n","                                                raiz+modelo_es_len25_50, 50, \"es\", df_espanol_dev.loc[df_espanol_dev[\"len_grupo\"] == \"entre25_50\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]],\n","                                [df_cruce_ingles.loc[df_cruce_ingles[\"len_grupo\"] == \"entre25_50\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],\n","                                 raiz+modelo_en_len25_50, 50, \"en\", df_ingles_dev.loc[df_ingles_dev[\"len_grupo\"] == \"entre25_50\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]],\n","                                [df_cruce_ingles.loc[df_cruce_ingles[\"len_grupo\"] == \"Menos25\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]],\n","                                 raiz+modelo_en_len0_25, 25, \"en\", df_ingles_dev.loc[df_ingles_dev[\"len_grupo\"] == \"Menos25\",[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]]):\n","    with open(log_file, \"a+\") as f:\n","      f.write(\";\".join([str(datetime.datetime.now()), suf, modelo, \"\\n\"]))\n","    if len(data) > 0:\n","      print(modelo)\n","      print(len(data))\n","      train_data, train_sampler, train_dataloader, tag2idx, device, tags_vals, idx2word, tr_inputs = bert_formating_one_dataset(data, MAX_LEN)\n","\n","      valid_data, valid_sampler, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs = bert_formating_one_dataset(data_dev, MAX_LEN)\n","      if os.path.exists(modelo):\n","        with open(log_file, \"a+\") as f:\n","          f.write(\"Escojo modelo existente: \" +  modelo +\"\\n\")\n","        model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= True, save_path = modelo)\n","      else:\n","        model, optimizer = model_creation(tag2idx, device, FULL_FINETUNING = True)\n","      training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = NUM_EPOCHS,\n","                  max_grad_norm = 1.0, save_path = modelo)\n","      pred_tags, valid_tags, valid_inputs = evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n","                  guarda_resultado=raiz+resultado+\"_validacion_\"+str(MAX_LEN)+\"_\"+lan+\".csv\")\n","      pred_tags, valid_tags, valid_inputs = evaluate(model, train_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n","                  guarda_resultado=raiz+resultado+\"_entrenamiento_\"+str(MAX_LEN)+\"_\"+lan+\".csv\")\n","    else:\n","      print(\"No hay datos\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":467},"id":"EV9FLpbCMC6b","executionInfo":{"status":"error","timestamp":1662072060786,"user_tz":-120,"elapsed":24298,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"4ce346ea-8097-482e-cae8-df1409d75a07"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/COLAB - TFM/100_bert_es_len_0_25\n","702\n","need more max_len - defect after filtering\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/200 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss: 1.7119653622309368\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/200 [00:14<?, ?it/s]\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-305f1a4b4b01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_creation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFULL_FINETUNING\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m       training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = NUM_EPOCHS,\n\u001b[0;32m---> 64\u001b[0;31m                   max_grad_norm = 1.0, save_path = modelo)\n\u001b[0m\u001b[1;32m     65\u001b[0m       pred_tags, valid_tags, valid_inputs = evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = modelo,\n\u001b[1;32m     66\u001b[0m                   guarda_resultado=raiz+resultado+\"_validacion_\"+str(MAX_LEN)+\"_\"+lan+\".csv\")\n","\u001b[0;32m<ipython-input-16-826896ba2464>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs, max_grad_norm, save_path)\u001b[0m\n\u001b[1;32m    356\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m               tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n\u001b[0;32m--> 358\u001b[0;31m                                     attention_mask=b_input_mask, labels=b_labels)\n\u001b[0m\u001b[1;32m    359\u001b[0m               logits = model(b_input_ids, token_type_ids=None,\n\u001b[1;32m    360\u001b[0m                             attention_mask=b_input_mask)\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, labels)\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0mactive_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1135\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: The shape of the mask [400] at index 0 does not match the shape of the indexed tensor [928] at index 0"]}]},{"cell_type":"code","source":["len(train_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnFH71FtBMbP","executionInfo":{"status":"ok","timestamp":1662072198836,"user_tz":-120,"elapsed":8,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"ce41386d-f62d-4a81-e2df-25b6621e7a8c"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["35"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":[],"metadata":{"id":"BgJD63uejDbN","executionInfo":{"status":"aborted","timestamp":1662068322094,"user_tz":-120,"elapsed":16,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}}},"execution_count":null,"outputs":[]}]}