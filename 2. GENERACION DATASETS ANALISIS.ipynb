{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["pqugLk_Lzyms"],"authorship_tag":"ABX9TyNSF5foqnAoVM8KaupDJJJ/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VVVo-N22oBEI","executionInfo":{"status":"ok","timestamp":1661893633276,"user_tz":-120,"elapsed":18146,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"340e6df0-ded3-41c9-92e8-2cbe679561d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 22.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.21.6)\n","Collecting boto3\n","  Downloading boto3-1.24.62-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 65.2 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.64.0)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2022.6.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (4.1.1)\n","Collecting botocore<1.28.0,>=1.27.62\n","  Downloading botocore-1.27.62-py3-none-any.whl (9.1 MB)\n","\u001b[K     |████████████████████████████████| 9.1 MB 74.5 MB/s \n","\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n","  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 9.5 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.28.0,>=1.27.62->boto3->pytorch-pretrained-bert) (2.8.2)\n","Collecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 73.3 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.28.0,>=1.27.62->boto3->pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 77.8 MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed boto3-1.24.62 botocore-1.27.62 jmespath-1.0.1 pytorch-pretrained-bert-0.6.2 s3transfer-0.6.0 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=db8efcaea90455a97cf1859e1afe8a0de8f94b29de2e9642b7584d0aa777fdf2\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting langdetect\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[K     |████████████████████████████████| 981 kB 30.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect) (1.15.0)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=fc989dc4375013e424bd1f8c8ad6150d214cb307bb636088a389d352ab0c8483\n","  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n","Successfully built langdetect\n","Installing collected packages: langdetect\n","Successfully installed langdetect-1.0.9\n"]}],"source":["# Instalaciones\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","!pip install langdetect\n"]},{"cell_type":"code","source":["# ANALISIS\n","import gensim"],"metadata":{"id":"RGpkL5nuip8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importaciones\n","# GENERAL Y PREPROCESADO\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","import spacy\n","!python -m spacy download es_core_news_sm\n","import es_core_news_sm\n","from langdetect import detect\n","import os.path\n","import datetime\n","# BERT\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","from seqeval.metrics import f1_score\n","\n","import gensim.downloader\n","word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObE8soCcoS9M","executionInfo":{"status":"ok","timestamp":1661894247228,"user_tz":-120,"elapsed":613090,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"1dd91e96-8600-4bb2-9b0d-1968ea9ff80f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2022-08-30 21:07:22.661472: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting es-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.4.0/es_core_news_sm-3.4.0-py3-none-any.whl (12.9 MB)\n","\u001b[K     |████████████████████████████████| 12.9 MB 29.8 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from es-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.64.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (8.1.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->es-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: es-core-news-sm\n","Successfully installed es-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","[=================================================-] 98.3% 1634.5/1662.8MB downloaded\n"]}]},{"cell_type":"code","source":["# Conexion a drive y descompresión de los corpus\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nu02Cu8roWKV","executionInfo":{"status":"ok","timestamp":1661894371868,"user_tz":-120,"elapsed":124647,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"12f0776a-287b-4ff3-bbf3-73c7bba3eea9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!unzip /content/drive/MyDrive/COLAB\\ -\\ TFM/corpora-master.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6itlXjaoZFp","executionInfo":{"status":"ok","timestamp":1661894372443,"user_tz":-120,"elapsed":582,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"e4b3265b-f212-4093-c1a7-3655f6caac63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/MyDrive/COLAB - TFM/corpora-master.zip\n","370a87a85d1d24d418a86abbe1f6918383362467\n","   creating: corpora-master/\n","  inflating: corpora-master/.gitignore  \n","   creating: corpora-master/.vscode/\n","  inflating: corpora-master/.vscode/settings.json  \n","   creating: corpora-master/2021/\n","  inflating: corpora-master/2021/README.md  \n","   creating: corpora-master/2021/eval/\n","   creating: corpora-master/2021/eval/develop/\n","  inflating: corpora-master/2021/eval/develop/README.md  \n","   creating: corpora-master/2021/eval/develop/scenario1-main/\n"," extracting: corpora-master/2021/eval/develop/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/develop/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/develop/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/develop/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/testing/\n","  inflating: corpora-master/2021/eval/testing/README.md  \n","   creating: corpora-master/2021/eval/testing/scenario1-main/\n"," extracting: corpora-master/2021/eval/testing/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/testing/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/testing/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/testing/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/eval/training/\n","   creating: corpora-master/2021/eval/training/scenario1-main/\n"," extracting: corpora-master/2021/eval/training/scenario1-main/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario1-main/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario2-taskA/\n"," extracting: corpora-master/2021/eval/training/scenario2-taskA/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/eval/training/scenario3-taskB/\n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/input.txt  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/eval/training/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/ref/\n","   creating: corpora-master/2021/ref/develop/\n","  inflating: corpora-master/2021/ref/develop/README.md  \n","  inflating: corpora-master/2021/ref/develop/cord.50.ann  \n","  inflating: corpora-master/2021/ref/develop/cord.50.txt  \n","  inflating: corpora-master/2021/ref/develop/medline.25.ann  \n","  inflating: corpora-master/2021/ref/develop/medline.25.txt  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.ann  \n","  inflating: corpora-master/2021/ref/develop/wikinews.25.txt  \n","   creating: corpora-master/2021/ref/testing/\n","  inflating: corpora-master/2021/ref/testing/README.md  \n","  inflating: corpora-master/2021/ref/testing/cord.150.ann  \n","  inflating: corpora-master/2021/ref/testing/cord.150.txt  \n","  inflating: corpora-master/2021/ref/testing/medline.75.ann  \n","  inflating: corpora-master/2021/ref/testing/medline.75.txt  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.ann  \n","  inflating: corpora-master/2021/ref/testing/wikinews.75.txt  \n","   creating: corpora-master/2021/ref/training/\n","  inflating: corpora-master/2021/ref/training/medline.1200.es.ann  \n","  inflating: corpora-master/2021/ref/training/medline.1200.es.txt  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.ann  \n","  inflating: corpora-master/2021/ref/training/wikinews.300.es.txt  \n","   creating: corpora-master/2021/submissions/\n","   creating: corpora-master/2021/submissions/baseline/\n","   creating: corpora-master/2021/submissions/baseline/develop/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/\n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/develop/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/2021/submissions/baseline/submission.zip  \n","   creating: corpora-master/2021/submissions/baseline/testing/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/\n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/testing/run1/scenario3-taskB/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/\n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario1-main/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario2-taskA/output.txt  \n","   creating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/\n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.ann  \n","  inflating: corpora-master/2021/submissions/baseline/training/run1/scenario3-taskB/output.txt  \n","  inflating: corpora-master/LICENSE  \n","  inflating: corpora-master/README.md  \n","   creating: corpora-master/scripts/\n","  inflating: corpora-master/scripts/README.md  \n","  inflating: corpora-master/scripts/anntools.py  \n","  inflating: corpora-master/scripts/baseline.py  \n","  inflating: corpora-master/scripts/score.py  \n","  inflating: corpora-master/scripts/score_2021.py  \n"]}]},{"cell_type":"markdown","source":["### Funciones"],"metadata":{"id":"pqugLk_Lzyms"}},{"cell_type":"code","source":["def clasifica_data(file):\n","  file_csv = file\n","  ingles = []\n","  espanol = []\n","  with open(file_csv, \"r\") as f:\n","    texto = f.read().split(\"\\n\")\n","  for frase in texto:\n","    espaciado = frase.split(\" \")\n","    idioma = detect(frase)\n","    if idioma == \"en\":\n","      ingles.append(frase)\n","    elif idioma == \"es\":\n","      espanol.append(frase)\n","  return ingles, espanol\n","\n","  \n","def data_preprocessing_es(texto, ann = None):\n","  nlp = es_core_news_sm.load()\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","def data_preprocessing_en(texto, ann = None):\n","  nlp = spacy.load(\"en_core_web_sm\")\n","  nlp_sentences = [nlp(x) for x in texto]\n","  list_registro = []\n","  for i, frase in enumerate(nlp_sentences):\n","    for j, token in enumerate(frase):\n","      list_registro.append([i, j, token.lower_, token.pos_])\n","  df_train = pd.DataFrame(list_registro)\n","  df_train.columns = [\"Sentence #\", \"Word In Sentence #\", \"Word\", \"POS\"]\n","  df_train = df_train[~df_train[\"Word\"].isin([\" \",\"\"])][df_train[\"POS\"] != \"PUNCT\"]\n","  if ann != None:\n","    with open(ann, \"r\") as f:\n","      texto_ann = [x.split(\"\\t\") for x in f.read().split(\"\\n\")]\n","    df_ann = pd.DataFrame(texto_ann)\n","    df_ann.columns = [\"num_termino\", \"desc\", \"Word\"]\n","    df_ann_terminos = df_ann[df_ann[\"num_termino\"].str.contains(\"T\")].copy()\n","    df_ann_terminos[\"TipoEntidad\"] = df_ann_terminos[\"desc\"].str.split(\" \").apply(lambda x: x[0])\n","    df_ann_terminos[\"Word_list\"] = df_ann_terminos[\"Word\"].apply(lambda x: list(nlp(x)))\n","    df_ann_terminos = df_ann_terminos.explode(\"Word_list\")\n","    df_ann_terminos[\"rn\"] = df_ann_terminos.groupby(\"num_termino\")[\"Word\"].cumcount()+1\n","    df_ann_terminos.loc[:,\"Tag\"]  = \"I-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"Tag\"] = \"B-\"+df_ann_terminos.loc[df_ann_terminos[\"rn\"] == 1,\"TipoEntidad\"]\n","    df_ann_terminos = df_ann_terminos[~df_ann_terminos[\"Word_list\"].isin([\" \",\"\"])]\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # Algoritmo de anotacion\n","    # Nos quedamos con un unico registro por numero de termino, palabra y tag\n","    df_ann_cruce = df_ann_terminos[[\"num_termino\",\"Word_list\",\"Tag\"]].drop_duplicates().reset_index(drop=True)\n","    df_ann_cruce.columns = [\"num_termino\",\"Word\", \"Tag\"]\n","    # generamos un flag unico\n","    df_ann_cruce[\"unico\"] = df_ann_cruce[\"num_termino\"] + df_ann_cruce[\"Word\"].\\\n","                              apply(lambda x: str(x))\n","    # inicializamos el pasado a NO\n","    df_ann_cruce[\"pasado\"] = \"NO\"\n","    # Inicializamos la ventana de busqueda\n","    ventana_busqueda = 20\n","    registro = []\n","    df_train_preproceso = df_train.copy()\n","    # Incializamos el ultimo termino encontrado a -1 para evitar descartar ningun termino de partida\n","    max_termino = -1\n","    # Iteramos por cada palabra del corpus y buscaremos en el dataframe de anotacion\n","    for num_linea, linea in enumerate(df_train_preproceso.iterrows()):\n","      palabra = linea[1][\"Word\"]\n","      # Definimos el dataframe de busqueda\n","      df_lookup = df_ann_cruce.head(ventana_busqueda)\n","      # Nos quedamos con el registro encontrado\n","      ann_match = df_lookup[df_lookup[\"Word\"].\\\n","                              apply(lambda x: str(x).lower()).\\\n","                              str.replace(\".\",\"\") == palabra.replace(\".\",\"\")].head(1)\n","      # if linea[1][\"Sentence #\"] == 724:\n","      #   print(palabra)\n","      #   print(df_lookup)\n","      #   print(ann_match)\n","      #   print(num_linea)\n","      #   print(df_ann_cruce)\n","      if len(ann_match) > 0:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                      linea[1][\"POS\"], ann_match[\"Tag\"].iloc[0], ann_match[\"num_termino\"].iloc[0]])\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"unico\"] != ann_match[\"unico\"].iloc[0],:]\n","        max_termino = ann_match[\"num_termino\"].iloc[0]\n","        # print(len(df_ann_cruce), print(len(registro)))\n","      else:\n","        registro.append([linea[1][\"Sentence #\"], linea[1][\"Word\"],\n","                          linea[1][\"POS\"], \"O\", \"NA\"])\n","      if num_linea % 100 == 0 and num_linea > 0:\n","        df_ann_cruce.loc[df_ann_cruce[\"num_termino\"].apply(lambda x: int(x[1:])) < int(max_termino[1:]),\"pasado\"] = \"SI\"\n","        df_ann_cruce = df_ann_cruce.loc[df_ann_cruce[\"pasado\"] != \"SI\",:]\n","    df = pd.DataFrame(registro)\n","    df.columns = [\"Sentence #\", \"Word\", \"POS\", \"Tag\", \"num_concepto\"]\n","    data = df[[\"Sentence #\", \"Word\", \"POS\", \"Tag\"]]\n","    MAX_LEN = int(data[\"Sentence #\"].value_counts().quantile(0.95))\n","    df_tmp = (data[\"Sentence #\"].value_counts() <= MAX_LEN)\n","    data = data[data[\"Sentence #\"].isin(df_tmp[df_tmp].index.tolist())]\n","    return data, MAX_LEN\n","  else:\n","    return df_train[[\"Word\", \"POS\"]]\n","\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, data):\n","        self.n_sent = 1\n","        self.data = data\n","        self.empty = False\n","        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n","                                                           s[\"POS\"].values.tolist(),\n","                                                           s[\"Tag\"].values.tolist())]\n","        self.grouped = self.data.groupby(\"Sentence #\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","def bert_formating(data, MAX_LEN, test_size=0.2):\n","  #concat sentence\n","  getter = SentenceGetter(data)\n","  word_list = [ [s[0] for s in sent] for sent in getter.sentences] \n","  sentences = word_list\n","  labels = [[s[2] for s in sent] for sent in getter.sentences]\n","  tags_vals = list(set(data[\"Tag\"].values))\n","  tag2idx = {t: i for i, t in enumerate(tags_vals)}\n","  idx2tag = {i: t for i, t in enumerate(tags_vals) }\n","  words = list(set(data[\"Word\"].values))\n","  n_words = len(words); \n","  word2idx = {w: i + 2 for i, w in enumerate(words)}\n","  word2idx[\"UNK\"] = 1\n","  word2idx[\"PAD\"] = 0\n","  idx2word = {i: w for w, i in word2idx.items()}\n","  bs = 16\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  if device == torch.device(\"cuda\"):\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0) \n","  tokenized_texts = word_list\n","  tokens_ids = [[word2idx[w] for w in s] for s in tokenized_texts]\n","  input_ids = pad_sequences(tokens_ids,\n","                          maxlen=int(MAX_LEN), dtype=\"int64\", truncating=\"post\", padding=\"post\")\n","  for i in tokens_ids:\n","    if len(i) > MAX_LEN:\n","        #print(tokens_ids)\n","        print(\"need more max_len - defect after filtering\")\n","        MAX_LEN = len(i)\n","  t_list = [[tag2idx.get(l) for l in lab] for lab in labels]\n","  tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"int64\", truncating=\"post\")\n","  attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n","  #split train test\n","  tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                              random_state=2022, test_size=test_size)\n","  tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                              random_state=2022, test_size=test_size)\n","  tr_inputs = torch.tensor(tr_inputs)\n","  val_inputs = torch.tensor(val_inputs)\n","  tr_tags = torch.tensor(tr_tags)\n","  val_tags = torch.tensor(val_tags)\n","  tr_masks = torch.tensor(tr_masks)\n","  val_masks = torch.tensor(val_masks)\n","  train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","  train_sampler = RandomSampler(train_data)\n","  train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","  return train_data, train_sampler, train_dataloader, valid_data, valid_sampler,\\\n","            valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def model_creation(tag2idx, device, FULL_FINETUNING = True, reentreno= False, save_path = \"\"):\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()\n","  if reentreno:\n","    print(\"Loading existing model...\")\n","    model.load_state_dict(torch.load(save_path))\n","  if FULL_FINETUNING:\n","      param_optimizer = list(model.named_parameters())\n","      no_decay = ['bias', 'gamma', 'beta']\n","      optimizer_grouped_parameters = [\n","          {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.01},\n","          {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","          'weight_decay_rate': 0.0}\n","      ]\n","  else:\n","      param_optimizer = list(model.classifier.named_parameters()) \n","      optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","  optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n","  return model, optimizer\n","\n","def training(model, optimizer, train_dataloader, valid_dataloader, tags_vals, idx2word, epochs = 200,\n","             max_grad_norm = 1.0, save_path = \"./bert1\"):\n","  train_f1 = []\n","  train_losses = []\n","  val_losses = []\n","  contador_max = -1\n","  for _ in trange(epochs, desc=\"Epoch\"):\n","      # TRAIN loop\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, batch in enumerate(train_dataloader):\n","          # add batch to gpu\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          # forward pass\n","          loss = model(b_input_ids, token_type_ids=None,\n","                      attention_mask=b_input_mask, labels=b_labels)\n","          # backward pass\n","          loss.backward()\n","          # track train loss\n","          tr_loss += loss.item()\n","          nb_tr_examples += b_input_ids.size(0)\n","          nb_tr_steps += 1\n","          # gradient clipping\n","          torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","          # update parameters\n","          optimizer.step()\n","          model.zero_grad()\n","      # print train loss per epoch\n","      print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","      # VALIDATION on validation set\n","      model.eval()\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels, true_inputs = [], [],[]\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","          \n","          with torch.no_grad():\n","              tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                    attention_mask=b_input_mask, labels=b_labels)\n","              logits = model(b_input_ids, token_type_ids=None,\n","                            attention_mask=b_input_mask)\n","          logits = logits.detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","          inputs = b_input_ids.to('cpu').numpy()\n","          \n","          true_inputs.append(inputs)\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.append(label_ids)\n","          \n","          tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","          \n","          eval_loss += tmp_eval_loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","          \n","          nb_eval_examples += b_input_ids.size(0)\n","          nb_eval_steps += 1\n","      eval_loss = eval_loss/nb_eval_steps\n","      print(\"Validation loss: {}\".format(eval_loss))\n","      print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","      pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","      valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","      valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","      f1 = f1_score([pred_tags], [valid_tags])\n","      train_f1.append(f1)\n","      train_losses.append(tr_loss/nb_tr_steps)\n","      val_losses.append(eval_loss)\n","\n","      max_f1 = max(train_f1)\n","      if f1 == max_f1:\n","        contador_max = 1\n","        torch.save(model.state_dict(), save_path)\n","      if contador_max > 0:\n","        contador_max += 1\n","      print(\"F1-Score: \" + str(train_f1[-1]))\n","      if round(max_f1, 2) > 0.1 and contador_max > 10 :\n","        print(\"Early stopping...\")\n","        return 0\n","def evaluate(model, valid_dataloader, tag2idx, device, tags_vals, idx2word, val_inputs, save_path = \"./bert1\",\n","             guarda_resultado=\"/content/drive/MyDrive/COLAB - TFM/resultado_entrenamiento1.csv\"):\n","  #evaluate model\n","  model = BertForTokenClassification.from_pretrained(u\"bert-base-uncased\", num_labels=len(tag2idx))\n","  model.load_state_dict(torch.load(save_path))\n","  if device == torch.device(\"cuda\"):\n","    model.cuda()  \n","  model.eval()\n","  predictions = []\n","  true_labels = []\n","  true_inputs = []\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","  print(len(valid_dataloader))\n","  for batch in tqdm(valid_dataloader):\n","      #print(len(batch))\n","      batch = tuple(t.to(device) for t in batch)\n","      b_input_ids, b_input_mask, b_labels = batch\n","\n","      with torch.no_grad():\n","          tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                attention_mask=b_input_mask, labels=b_labels)\n","          logits = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask)\n","          \n","      logits = logits.detach().cpu().numpy()\n","      predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","      label_ids = b_labels.to('cpu').numpy()\n","      inputs = b_input_ids.to('cpu').numpy()\n","      true_inputs.append(inputs)\n","      \n","      \n","      true_labels.append(label_ids)\n","      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","      eval_loss += tmp_eval_loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","      nb_eval_examples += b_input_ids.size(0)\n","      nb_eval_steps += 1\n","\n","  pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n","  valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n","  valid_inputs = [[idx2word[l_ii] for l_ii in l_i] for l in  true_inputs  for l_i in l ]\n","\n","  count = 0 \n","  all_data = 0\n","  registro_resultado = []\n","  for i,j,ll in zip(pred_tags,valid_tags,val_inputs):\n","      for k,l,kk in zip(i,j,ll):\n","        count += 1\n","        registro_resultado.append([k,l,idx2word[kk.item()]])\n","        all_data += 1\n","  df_alldata = pd.DataFrame(registro_resultado)\n","  df_alldata.columns = [\"pred_tags\", \"valid_tags\", \"val_inputs\"]\n","  df_alldata[df_alldata[\"val_inputs\"] != \"PAD\"].\\\n","        to_csv(guarda_resultado,sep=\";\", encoding = \"utf-8\")\n","  return pred_tags, valid_tags, valid_inputs\n","\n","def aplica_wv(x):\n","  try:\n","    res = word2vec_vectors[x]\n","  except:\n","    res = np.zeros(300)\n","  return res\n"],"metadata":{"id":"wEwSrIk1oaaO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Análisis"],"metadata":{"id":"R2oHlmapz7Dk"}},{"cell_type":"code","source":["corpus_anotacion = ([\"/content/corpora-master/2021/ref/training/wikinews.300.es.txt\", \"/content/corpora-master/2021/ref/training/wikinews.300.es.ann\"],\n","                    [\"/content/corpora-master/2021/ref/training/medline.1200.es.txt\", \"/content/corpora-master/2021/ref/training/medline.1200.es.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/cord.150.txt\", \"/content/corpora-master/2021/ref/testing/cord.150.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/medline.75.txt\", \"/content/corpora-master/2021/ref/testing/medline.75.ann\"],\n","                    [\"/content/corpora-master/2021/ref/testing/wikinews.75.txt\", \"/content/corpora-master/2021/ref/testing/wikinews.75.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/cord.50.txt\", \"/content/corpora-master/2021/ref/develop/cord.50.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/medline.25.txt\", \"/content/corpora-master/2021/ref/develop/medline.25.ann\"],\n","                    [\"/content/corpora-master/2021/ref/develop/wikinews.25.txt\", \"/content/corpora-master/2021/ref/develop/wikinews.25.ann\"],\n","                    )\n"],"metadata":{"id":"p90UG2axtG3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raiz = \"/content/drive/MyDrive/COLAB - TFM/\"\n","log_file = \"/content/drive/MyDrive/COLAB - TFM/full_training.log\""],"metadata":{"id":"OFDFqjRAg7RI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus, anotacion = corpus_anotacion[0]\n","resultado = corpus.split(\"/\")[-1]\n","print(raiz+resultado+\"_analisis_oraciones.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5q-HmqVhvae","executionInfo":{"status":"ok","timestamp":1661894372903,"user_tz":-120,"elapsed":11,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"d19de9dc-63a4-404e-a7ba-4998b6457df7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/COLAB - TFM/wikinews.300.es.txt_analisis_oraciones.csv\n"]}]},{"cell_type":"code","source":["for corpus, anotacion in corpus_anotacion:\n","  resultado = corpus.split(\"/\")[-1]\n","  ingles, espanol = clasifica_data(file=corpus)\n","  if len(ingles) > 0:\n","    df_ingles_analiza, MAX_LEN = data_preprocessing_en(ingles, ann = anotacion)\n","    # Longitud de la oracion\n","    df_tmp = df_ingles_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","    df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_ingles_analiza, df_tmp[[\"Sentence #\", \"longitud\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    # Repeticion de las palabras en el corpus para calcular pesos\n","    df_tmp = df_ingles_analiza.groupby(\"Word\").POS.count().reset_index()\n","    df_tmp[\"rep_palabras\"] = df_tmp[\"POS\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Word\", \"rep_palabras\"]],\n","                                on=[\"Word\"], how=\"left\")\n","    df_cruce_ingles[\"palabra_ratio\"] = df_cruce_ingles[\"rep_palabras\"]/len(df_cruce_ingles)\n","    # Repeticion de las POS en el corpus para calcular pesos\n","    df_tmp = df_ingles_analiza.groupby(\"POS\").Word.count().reset_index()\n","    df_tmp[\"rep_pos\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"POS\", \"rep_pos\"]],\n","                                on=[\"POS\"], how=\"left\")\n","    df_cruce_ingles[\"pos_ratio\"] = df_cruce_ingles[\"rep_pos\"]/len(df_cruce_ingles)\n","    df_cruce_ingles[\"freq_palabra\"] = df_cruce_ingles[\"palabra_ratio\"] < df_cruce_ingles[\"palabra_ratio\"].quantile(0.33)\n","    df_cruce_ingles[\"freq_pos\"] = df_cruce_ingles[\"pos_ratio\"] < df_cruce_ingles[\"pos_ratio\"].quantile(0.33)\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_palabra.sum().reset_index()\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_palabra.sum().reset_index().sort_values(\"freq_palabra\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_palabra\", \"Sentence #\", \"freq_palabra\"]\n","    df_tmp[\"sum_freq_palabra\"] = df_tmp[\"freq_palabra\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"sum_freq_palabra\", \"rank_freq_palabra\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").freq_pos.sum().reset_index().sort_values(\"freq_pos\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_pos\", \"Sentence #\", \"freq_pos\"]\n","    df_tmp[\"sum_freq_pos\"] = df_tmp[\"freq_pos\"].astype(int)\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"sum_freq_pos\", \"rank_freq_pos\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_ingles[\"wv\"] = df_cruce_ingles[\"Word\"].apply(aplica_wv)\n","    df_tmp = df_cruce_ingles.groupby(\"Sentence #\").wv.mean().apply(np.linalg.norm).reset_index()\n","    df_tmp[\"dif_wv_33\"] = df_tmp[\"wv\"] -  df_tmp[\"wv\"].median()\n","    df_cruce_ingles = pd.merge(df_cruce_ingles, df_tmp[[\"Sentence #\", \"dif_wv_33\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_ingles[[\"Sentence #\", 'longitud', 'sum_freq_palabra', 'rank_freq_palabra', 'sum_freq_pos', 'rank_freq_pos', 'dif_wv_33']].drop_duplicates().to_csv(raiz+resultado+\"_analisis_oraciones.csv\")\n","    df_cruce_ingles[[\"Sentence #\",\t\"Word\",\t\"POS\",\t\"Tag\",\t\"longitud\"]].to_csv(raiz+resultado+\"_preprocessing.csv\")\n","  if len(espanol) > 0:\n","    df_espanol_analiza, MAX_LEN = data_preprocessing_es(espanol, ann = anotacion)\n","    # Longitud de la oracion\n","    df_tmp = df_espanol_analiza.groupby(\"Sentence #\").Word.count().reset_index()\n","    df_tmp[\"longitud\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_espanol_analiza, df_tmp[[\"Sentence #\", \"longitud\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    # Repeticion de las palabras en el corpus para calcular pesos\n","    df_tmp = df_espanol_analiza.groupby(\"Word\").POS.count().reset_index()\n","    df_tmp[\"rep_palabras\"] = df_tmp[\"POS\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Word\", \"rep_palabras\"]],\n","                                on=[\"Word\"], how=\"left\")\n","    df_cruce_espanol[\"palabra_ratio\"] = df_cruce_espanol[\"rep_palabras\"]/len(df_cruce_espanol)\n","    # Repeticion de las POS en el corpus para calcular pesos\n","    df_tmp = df_espanol_analiza.groupby(\"POS\").Word.count().reset_index()\n","    df_tmp[\"rep_pos\"] = df_tmp[\"Word\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"POS\", \"rep_pos\"]],\n","                                on=[\"POS\"], how=\"left\")\n","    df_cruce_espanol[\"pos_ratio\"] = df_cruce_espanol[\"rep_pos\"]/len(df_cruce_espanol)\n","    df_cruce_espanol[\"freq_palabra\"] = df_cruce_espanol[\"palabra_ratio\"] < df_cruce_espanol[\"palabra_ratio\"].quantile(0.33)\n","    df_cruce_espanol[\"freq_pos\"] = df_cruce_espanol[\"pos_ratio\"] < df_cruce_espanol[\"pos_ratio\"].quantile(0.33)\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_palabra.sum().reset_index()\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_palabra.sum().reset_index().sort_values(\"freq_palabra\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_palabra\", \"Sentence #\", \"freq_palabra\"]\n","    df_tmp[\"sum_freq_palabra\"] = df_tmp[\"freq_palabra\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"sum_freq_palabra\", \"rank_freq_palabra\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").freq_pos.sum().reset_index().sort_values(\"freq_pos\", ascending=False).reset_index(drop=True).reset_index()\n","    df_tmp.columns = [\"rank_freq_pos\", \"Sentence #\", \"freq_pos\"]\n","    df_tmp[\"sum_freq_pos\"] = df_tmp[\"freq_pos\"].astype(int)\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"sum_freq_pos\", \"rank_freq_pos\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_espanol[\"wv\"] = df_cruce_espanol[\"Word\"].apply(aplica_wv)\n","    df_tmp = df_cruce_espanol.groupby(\"Sentence #\").wv.mean().apply(np.linalg.norm).reset_index()\n","    df_tmp[\"dif_wv_33\"] = df_tmp[\"wv\"] -  df_tmp[\"wv\"].median()\n","    df_cruce_espanol = pd.merge(df_cruce_espanol, df_tmp[[\"Sentence #\", \"dif_wv_33\"]],\n","                                on=[\"Sentence #\"], how=\"left\")\n","    df_cruce_espanol[[\"Sentence #\", 'longitud', 'sum_freq_palabra', 'rank_freq_palabra', 'sum_freq_pos', 'rank_freq_pos', 'dif_wv_33']].drop_duplicates().to_csv(raiz+resultado+\"_analisis_oraciones.csv\")\n","    df_cruce_espanol[[\"Sentence #\",\t\"Word\",\t\"POS\",\t\"Tag\",\t\"longitud\"]].to_csv(raiz+resultado+\"_preprocessing.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j_BuQgUjg8RN","executionInfo":{"status":"ok","timestamp":1661894706288,"user_tz":-120,"elapsed":123249,"user":{"displayName":"Luis Esteban Andaluz","userId":"00193003036453540000"}},"outputId":"accf3b42-43b1-4636-89c1-f8e495d07984"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Y6tDSWW6hzYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"BgJD63uejDbN"},"execution_count":null,"outputs":[]}]}